#!/usr/bin/env python3
import cv2
import json
import time
import math
import numpy as np
import pyrealsense2 as rs
from datetime import datetime
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Any
from pymycobot import MyCobot280
from camera import capture_d455_images, load_intrinsics


class EyeToHandCalibrator:
    """
    Eye-to-Hand calibrationì„ ìˆ˜í–‰í•˜ëŠ” í´ë˜ìŠ¤
    
    ChArUco ë³´ë“œë¥¼ End-Effectorì— ê³ ì •í•˜ê³  ë‹¤ì–‘í•œ í¬ì¦ˆì—ì„œ ì´ë¯¸ì§€ë¥¼ ìº¡ì²˜í•˜ì—¬
    ì™¸ë¶€ ê³ ì • ì¹´ë©”ë¼ì™€ ë¡œë´‡ End-Effector ê°„ì˜ ë³€í™˜ í–‰ë ¬ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, 
                 robot_port: str = "/dev/ttyACM0",
                 robot_baud: int = 115200,
                 save_dir: str = "/home/ros/llm_robot/data/Calibration/Eye-to-Hand2",
                 charuco_squares_x: int = 7,   # ê°€ë¡œ ì‚¬ê°í˜• ê°œìˆ˜ (columns)
                 charuco_squares_y: int = 5,   # ì„¸ë¡œ ì‚¬ê°í˜• ê°œìˆ˜ (rows)
                 charuco_square_length: float = 28.0,  # mm
                 charuco_marker_length: float = 14.0,  # mm
                 min_charuco_corners: int = 25,  # ìµœì†Œ ChArUco ì½”ë„ˆ ê°œìˆ˜
                 min_detection_confidence: float = 0.5,  # ìµœì†Œ ê²€ì¶œ ì‹ ë¢°ë„
                 max_distance_threshold: float = 0.7):  # ìµœëŒ€ ê±°ë¦¬ ì„ê³„ê°’ (ë¯¸í„°)
        """
        Eye-to-Hand calibration ì´ˆê¸°í™”
        
        Args:
            robot_port: ë¡œë´‡ ì‹œë¦¬ì–¼ í¬íŠ¸
            robot_baud: ë¡œë´‡ í†µì‹  ë³´ë“œë ˆì´íŠ¸
            save_dir: ìº¡ì²˜ëœ ì´ë¯¸ì§€ì™€ í¬ì¦ˆ ë°ì´í„° ì €ì¥ ë””ë ‰í† ë¦¬
            charuco_squares_x: ChArUco ë³´ë“œ ê°€ë¡œ ì‚¬ê°í˜• ê°œìˆ˜
            charuco_squares_y: ChArUco ë³´ë“œ ì„¸ë¡œ ì‚¬ê°í˜• ê°œìˆ˜
            charuco_square_length: ChArUco ì‚¬ê°í˜• í•œ ë³€ì˜ ê¸¸ì´ (mm)
            charuco_marker_length: ChArUco ë§ˆì»¤ í•œ ë³€ì˜ ê¸¸ì´ (mm)
        """
        self.robot_port = robot_port
        self.robot_baud = robot_baud
        self.save_dir = Path(save_dir)
        self.poses_dir = self.save_dir / "poses"
        self.angles_dir = self.save_dir / "angles"
        
        # ChArUco ë³´ë“œ íŒŒë¼ë¯¸í„°
        self.charuco_squares_x = charuco_squares_x
        self.charuco_squares_y = charuco_squares_y
        self.charuco_square_length = charuco_square_length  # mm ë‹¨ìœ„ ìœ ì§€
        self.charuco_marker_length = charuco_marker_length  # mm ë‹¨ìœ„ ìœ ì§€
        
        # ì´ìƒì¹˜ ì²˜ë¦¬ íŒŒë¼ë¯¸í„°
        self.min_charuco_corners = min_charuco_corners
        self.min_detection_confidence = min_detection_confidence
        self.max_distance_threshold = max_distance_threshold
        
        # ChArUco ë”•ì…”ë„ˆë¦¬ ë° ë³´ë“œ ìƒì„±
        self.aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_6X6_250)
        self.charuco_board = cv2.aruco.CharucoBoard(
            size=(charuco_squares_x, charuco_squares_y),  # (ê°€ë¡œ, ì„¸ë¡œ) ìˆœì„œë¡œ ìˆ˜ì •
            squareLength=self.charuco_square_length / 1000.0,  # mmë¥¼ më¡œ ë³€í™˜
            markerLength=self.charuco_marker_length / 1000.0,  # mmë¥¼ më¡œ ë³€í™˜
            dictionary=self.aruco_dict
        )
        
        # ë¡œë´‡ ì—°ê²°
        self.robot = MyCobot280(robot_port, robot_baud)
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.save_dir.mkdir(parents=True, exist_ok=True)
        self.poses_dir.mkdir(parents=True, exist_ok=True)
        self.angles_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"Eye-to-Hand Calibrator ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"ChArUco í¬ê¸°: {charuco_squares_x}x{charuco_squares_y}")
        print(f"ì‚¬ê°í˜• ê¸¸ì´: {charuco_square_length:.1f}mm")
        print(f"ë§ˆì»¤ ê¸¸ì´: {charuco_marker_length:.1f}mm")
        print(f"Eye-to-Hand calibration ëª¨ë“œ")
        print(f"ì´ìƒì¹˜ ì²˜ë¦¬ íŒŒë¼ë¯¸í„°:")
        print(f"  - ìµœì†Œ ChArUco ì½”ë„ˆ: {min_charuco_corners}ê°œ")
        print(f"  - ìµœì†Œ ê²€ì¶œ ì‹ ë¢°ë„: {min_detection_confidence}")
        print(f"  - ìµœëŒ€ ê±°ë¦¬ ì„ê³„ê°’: {max_distance_threshold}m")
        print(f"ì¢Œí‘œì¶• ë³€í™˜:")
        print(f"  - ì¹´ë©”ë¼ Zì¶• â†’ ë¡œë´‡ Xì¶•")
        print(f"  - ì¹´ë©”ë¼ -Xì¶• â†’ ë¡œë´‡ Yì¶•")
        print(f"  - ì¹´ë©”ë¼ -Yì¶• â†’ ë¡œë´‡ Zì¶•")
    
    def _load_camera_intrinsics(self, intrinsics_path: str) -> Tuple[np.ndarray, np.ndarray]:
        """
        ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        
        Args:
            intrinsics_path: ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„° JSON íŒŒì¼ ê²½ë¡œ
            
        Returns:
            Tuple[np.ndarray, np.ndarray]: (camera_matrix, dist_coeffs)
        """
        try:
            # ìƒˆë¡œìš´ í˜•ì‹ì˜ intrinsics íŒŒì¼ ë¡œë“œ
            with open(intrinsics_path, 'r') as f:
                intrinsics_data = json.load(f)
            
            # camera_matrixì™€ dist_coeffsê°€ ì§ì ‘ ì €ì¥ëœ ê²½ìš°
            if "camera_matrix" in intrinsics_data and "dist_coeffs" in intrinsics_data:
                camera_matrix = np.array(intrinsics_data["camera_matrix"], dtype=np.float64)
                dist_coeffs = np.array(intrinsics_data["dist_coeffs"], dtype=np.float64)
                print(f"ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„° ë¡œë“œ ì™„ë£Œ: {intrinsics_path}")
                return camera_matrix, dist_coeffs
            
            # ê¸°ì¡´ í˜•ì‹ì˜ intrinsics íŒŒì¼ ë¡œë“œ (load_intrinsics í•¨ìˆ˜ ì‚¬ìš©)
            camera_matrix, dist_coeffs = load_intrinsics(intrinsics_path)
            print(f"ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„° ë¡œë“œ ì™„ë£Œ: {intrinsics_path}")
            return camera_matrix, dist_coeffs
            
        except Exception as e:
            print(f"ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ì¹´ë©”ë¼ í–‰ë ¬ ì‚¬ìš© (RealSense D455 848x480 í•´ìƒë„ìš©)
            camera_matrix = np.array([
                [430.341,   0.0,     422.633],
                [0.0,       430.341, 244.632],
                [0.0,       0.0,     1.0]
            ], dtype=np.float64)
            
            # ì™œê³¡ ê³„ìˆ˜ (k1, k2, p1, p2, k3)
            dist_coeffs = np.array([
                -0.05594644322991371,
                0.06878077983856201,
                -0.00011232726683374494,
                0.000743341923225671,
                -0.022005939856171608
            ], dtype=np.float64)
            return camera_matrix, dist_coeffs
    


    def detect_charuco_pose(self, image: np.ndarray, camera_matrix: np.ndarray, dist_coeffs: np.ndarray) -> Optional[Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]]:
        """
        ì´ë¯¸ì§€ì—ì„œ ChArUco ë³´ë“œì˜ í¬ì¦ˆë¥¼ ê²€ì¶œí•©ë‹ˆë‹¤.
        cv_test.ipynbì˜ ì½”ë“œë¥¼ ì°¸ê³ í•˜ì—¬ OpenCV 4.11.0ì— ë§ê²Œ ê°œì„ ëœ ë²„ì „
        """
        # 1. ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (CLAHEë¡œ ëŒ€ë¹„ í–¥ìƒ)
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
        gray_enhanced = clahe.apply(gray)
        
        # ë³´ë“œ ì„¤ì • ì •ë³´ ì¶œë ¥
        # print(f"\rDEBUG: Board config - size: ({self.charuco_squares_x}, {self.charuco_squares_y}), square_length: {self.charuco_square_length:.1f}mm, marker_length: {self.charuco_marker_length:.1f}mm", end="")
        
        # 2. ChArUco ë””í…í„° ìƒì„± (OpenCV 4.11.0 ë°©ì‹)
        charuco_detector = cv2.aruco.CharucoDetector(self.charuco_board)
        
        # 3. ChArUco ì½”ë„ˆ ë° ë§ˆì»¤ ê²€ì¶œ (ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ì‚¬ìš©)
        charuco_corners, charuco_ids, marker_corners, marker_ids = charuco_detector.detectBoard(gray_enhanced)
        
        print(f"\rDEBUG: ChArUco corners: {len(charuco_corners) if charuco_corners is not None else 0}, IDs: {len(charuco_ids) if charuco_ids is not None else 0}, markers: {len(marker_corners) if marker_corners is not None else 0}", end="")
        
        # 4. ChArUco ì½”ë„ˆê°€ ì¶©ë¶„íˆ ê²€ì¶œëœ ê²½ìš° pose ì¶”ì • (ìµœì†Œ 6ê°œ í•„ìš”)
        if (charuco_corners is not None and charuco_ids is not None and 
            len(charuco_corners) >= 6 and len(charuco_corners) == len(charuco_ids)):
            
            print(f"\rDEBUG: ChArUco corners detected: {len(charuco_corners)}, proceeding with pose estimation", end="")
            
            # 5. 2D-3D ëŒ€ì‘ì  ì¶”ì¶œ (cv_test.ipynb ë°©ì‹)
            objPoints, imgPoints = self.charuco_board.matchImagePoints(charuco_corners, charuco_ids)
            
            if objPoints is not None and imgPoints is not None and len(objPoints) >= 6:
                # 6. solvePnPë¥¼ í†µí•´ ë³´ë“œì˜ 3D í¬ì¦ˆ ê³„ì‚°
                success, rvec, tvec = cv2.solvePnP(
                    objPoints,
                    imgPoints,
                    camera_matrix,
                    dist_coeffs,
                    flags=cv2.SOLVEPNP_ITERATIVE
                )
                
                if success:
                    print(f"\rDEBUG: ChArUco pose estimation successful with {len(objPoints)} points", end="")
                    return rvec, tvec, charuco_corners, charuco_ids
                else:
                    print(f"\rDEBUG: solvePnP failed for ChArUco corners", end="")
            else:
                print(f"\rDEBUG: Not enough 2D-3D correspondences ({len(objPoints) if objPoints is not None else 0} < 6)", end="")
        else:
            corner_count = len(charuco_corners) if charuco_corners is not None else 0
            if corner_count < 6:
                print(f"\rDEBUG: Skipping pose estimation, only {corner_count} corners detected (need >= 6)", end="")
            else:
                print(f"\rDEBUG: ChArUco detection failed, trying ArUco marker-based fallback", end="")
        
        # 7. ChArUco ê²€ì¶œ ì‹¤íŒ¨ ì‹œ ArUco ë§ˆì»¤ ê¸°ë°˜ fallback
        print(f"\rDEBUG: ChArUco detection failed, trying ArUco marker-based fallback", end="")
        
        # ArUco ë§ˆì»¤ ê²€ì¶œ
        aruco_params = cv2.aruco.DetectorParameters()
        aruco_params.adaptiveThreshWinSizeMin = 3
        aruco_params.adaptiveThreshWinSizeMax = 23
        aruco_params.adaptiveThreshWinSizeStep = 10
        aruco_params.adaptiveThreshConstant = 7
        aruco_params.minMarkerPerimeterRate = 0.03
        aruco_params.maxMarkerPerimeterRate = 4.0
        aruco_params.polygonalApproxAccuracyRate = 0.03
        aruco_params.minCornerDistanceRate = 0.05
        aruco_params.minMarkerDistanceRate = 0.05
        aruco_params.minDistanceToBorder = 3
        aruco_params.minOtsuStdDev = 5.0
        aruco_params.perspectiveRemovePixelPerCell = 4
        aruco_params.perspectiveRemoveIgnoredMarginPerCell = 0.13
        aruco_params.maxErroneousBitsInBorderRate = 0.35
        aruco_params.errorCorrectionRate = 0.6
        
        aruco_detector = cv2.aruco.ArucoDetector(self.aruco_dict, aruco_params)
        corners, ids, rejected = aruco_detector.detectMarkers(gray_enhanced)
        
        # ArUco ë§ˆì»¤ ê²€ì¶œ ê²°ê³¼ í™•ì¸
        if ids is not None and len(ids) >= 10:  # ìµœì†Œ 10ê°œ ë§ˆì»¤ í•„ìš”
            print(f"\rDEBUG: ArUco markers detected: {len(ids)}, proceeding with marker-based pose estimation", end="")
            
            # ArUco ë§ˆì»¤ì˜ 3D ì ë“¤ ìƒì„± (ë³´ë“œ ì¢Œí‘œê³„, ë¯¸í„° ë‹¨ìœ„)
            marker_points_3d = []
            marker_points_2d = []
            
            for i, marker_id in enumerate(ids):
                marker_id = marker_id[0]  # idsëŠ” 2D ë°°ì—´
                
                # ë§ˆì»¤ì˜ 4ê°œ ì½”ë„ˆë¥¼ 3D ì ìœ¼ë¡œ ë³€í™˜
                # ë§ˆì»¤ì˜ ì¤‘ì‹¬ ìœ„ì¹˜ ê³„ì‚° (ë³´ë“œ ì¢Œí‘œê³„)
                marker_row = marker_id // (self.charuco_squares_x - 1)
                marker_col = marker_id % (self.charuco_squares_x - 1)
                
                # ë§ˆì»¤ ì¤‘ì‹¬ì˜ 3D ì¢Œí‘œ (ë¯¸í„° ë‹¨ìœ„)
                center_x = marker_col * (self.charuco_square_length / 1000.0)  # mm -> m
                center_y = marker_row * (self.charuco_square_length / 1000.0)  # mm -> m
                
                # ë§ˆì»¤ì˜ 4ê°œ ì½”ë„ˆë¥¼ 3D ì ìœ¼ë¡œ ìƒì„± (ë¯¸í„° ë‹¨ìœ„)
                half_marker = (self.charuco_marker_length / 1000.0) / 2  # mm -> m
                marker_corners_3d = [
                    [center_x - half_marker, center_y - half_marker, 0],  # ì¢Œìƒë‹¨
                    [center_x + half_marker, center_y - half_marker, 0],  # ìš°ìƒë‹¨
                    [center_x + half_marker, center_y + half_marker, 0],  # ìš°í•˜ë‹¨
                    [center_x - half_marker, center_y + half_marker, 0]   # ì¢Œí•˜ë‹¨
                ]
                
                # 2D ì½”ë„ˆ ì ë“¤
                marker_corners_2d = corners[i][0]  # corners[i]ëŠ” 4x2 ë°°ì—´
                
                # 3D-2D ë§¤í•‘ì— ì¶”ê°€
                marker_points_3d.extend(marker_corners_3d)
                marker_points_2d.extend(marker_corners_2d)
            
            if len(marker_points_3d) >= 12:  # ìµœì†Œ 3ê°œ ë§ˆì»¤ (3*4=12ê°œ ì )
                marker_points_3d = np.array(marker_points_3d, dtype=np.float32)
                marker_points_2d = np.array(marker_points_2d, dtype=np.float32)
                
                # solvePnPë¡œ í¬ì¦ˆ ì¶”ì •
                ret, rvec, tvec = cv2.solvePnP(
                    marker_points_3d, marker_points_2d,
                    camera_matrix, dist_coeffs,
                    flags=cv2.SOLVEPNP_ITERATIVE
                )
                
                if ret:
                    print(f"\rDEBUG: ArUco marker-based pose estimation successful with {len(ids)} markers", end="")
                    
                    # ArUco ë§ˆì»¤ ê¸°ë°˜ ê²°ê³¼ë¥¼ charuco_corners í˜•íƒœë¡œ ë³€í™˜
                    # ë§ˆì»¤ ì¤‘ì‹¬ì ë“¤ì„ charuco_cornersë¡œ ì‚¬ìš©
                    charuco_corners_from_markers = []
                    charuco_ids_from_markers = []
                    
                    for i, marker_id in enumerate(ids):
                        marker_id = marker_id[0]  # idsëŠ” 2D ë°°ì—´
                        marker_corners = corners[i][0]  # 4x2 ë°°ì—´
                        
                        # ë§ˆì»¤ì˜ ì¤‘ì‹¬ì  ê³„ì‚°
                        center_2d = np.mean(marker_corners, axis=0)
                        charuco_corners_from_markers.append(center_2d)
                        charuco_ids_from_markers.append(marker_id)
                    
                    charuco_corners_from_markers = np.array(charuco_corners_from_markers, dtype=np.float32)
                    charuco_ids_from_markers = np.array(charuco_ids_from_markers, dtype=np.int32)
                    
                    # ArUco ë§ˆì»¤ ê¸°ë°˜ ê²°ê³¼ë¥¼ ë°˜í™˜ (charuco_corners í˜•íƒœë¡œ ë³€í™˜)
                    return rvec, tvec, charuco_corners_from_markers, charuco_ids_from_markers
                else:
                    print(f"\rDEBUG: ArUco marker-based pose estimation failed", end="")
            else:
                print(f"\rDEBUG: Not enough 3D-2D correspondences for ArUco markers ({len(marker_points_3d)} < 12)", end="")
        else:
            print(f"\rDEBUG: Not enough ArUco markers detected ({len(ids) if ids is not None else 0} < 10)", end="")
        
        return None
    
    def get_robot_pose(self) -> Tuple[np.ndarray, np.ndarray]:
        """
        í˜„ì¬ ë¡œë´‡ì˜ í¬ì¦ˆë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        
        Returns:
            Tuple[np.ndarray, np.ndarray]: (R_base2ee, t_base2ee) - ë¯¸í„° ë‹¨ìœ„
        """
        coords = self.robot.get_coords()  # [X, Y, Z, Rx, Ry, Rz] (mm, degree)
        
        # ìœ„ì¹˜ (mmë¥¼ ë¯¸í„°ë¡œ ë³€í™˜)
        position = np.array(coords[0:3]) / 1000.0  # mm -> m
        
        # íšŒì „ (ë„ë¥¼ ë¼ë””ì•ˆìœ¼ë¡œ ë³€í™˜ í›„ íšŒì „ í–‰ë ¬ ê³„ì‚°)
        rx, ry, rz = np.radians(coords[3:6])  # degree -> radian
        R_base2ee = self._euler_to_rotation_matrix(rx, ry, rz)
        
        return R_base2ee, position.reshape(3, 1)
    
    def get_robot_angles(self) -> List[float]:
        """
        í˜„ì¬ ë¡œë´‡ì˜ ê°ë„ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        
        Returns:
            List[float]: 6ê°œ ì¡°ì¸íŠ¸ì˜ ê°ë„ (ë„)
        """
        return self.robot.get_angles()
    
    def evaluate_charuco_quality(self, charuco_corners: np.ndarray, charuco_ids: np.ndarray, 
                                rvec: np.ndarray, tvec: np.ndarray) -> Tuple[bool, str, float]:
        """
        ChArUco ê²€ì¶œ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.
        
        Args:
            charuco_corners: ê²€ì¶œëœ ChArUco ì½”ë„ˆ
            charuco_ids: ê²€ì¶œëœ ChArUco ID
            rvec: íšŒì „ ë²¡í„°
            tvec: ë³€ìœ„ ë²¡í„°
            
        Returns:
            Tuple[bool, str, float]: (í’ˆì§ˆ í†µê³¼ ì—¬ë¶€, í‰ê°€ ë©”ì‹œì§€, í’ˆì§ˆ ì ìˆ˜)
        """
        quality_score = 0.0
        issues = []
        
        # 1. ì½”ë„ˆ ê°œìˆ˜ ê²€ì‚¬ (ì•ˆì „í•œ ì²˜ë¦¬)
        corner_count = len(charuco_corners) if charuco_corners is not None else 0
        print(f"\rDEBUG: Evaluating quality - corners: {corner_count}, min_required: {self.min_charuco_corners}", end="")
        
        if charuco_corners is None or corner_count < self.min_charuco_corners:
            issues.append(f"ì½”ë„ˆ ê°œìˆ˜ ë¶€ì¡± ({corner_count}/{self.min_charuco_corners})")
            quality_score -= 0.1
            print(f"\rDEBUG: Corner count penalty applied, score: {quality_score:.2f}", end="")
        else:
            corner_ratio = corner_count / ((self.charuco_squares_x - 1) * (self.charuco_squares_y - 1))
            quality_score += corner_ratio * 0.6
            print(f"\rDEBUG: Corner ratio: {corner_ratio:.2f}, score: {quality_score:.2f}", end="")
        
        # 2. ê±°ë¦¬ ê²€ì‚¬
        distance = np.linalg.norm(tvec)
        if distance > self.max_distance_threshold:
            issues.append(f"ê±°ë¦¬ ë„ˆë¬´ ë©€ìŒ ({distance*1000:.1f}mm > {self.max_distance_threshold*1000:.1f}mm)")
            quality_score -= 0.2
        elif distance < 0.15:
            issues.append(f"ê±°ë¦¬ ë„ˆë¬´ ê°€ê¹Œì›€ ({distance*1000:.1f}mm < 150mm)")
            quality_score -= 0.1
        elif distance > 0.6:
            issues.append(f"ê±°ë¦¬ ë„ˆë¬´ ë©€ìŒ ({distance*1000:.1f}mm > 500mm)")
            quality_score -= 0.2
        else:
            distance_score = 1.0 - (distance - 0.15) / (0.5 - 0.15)
            quality_score += distance_score * 0.3
        
        # 3. íšŒì „ ê°ë„ ê²€ì‚¬
        rotation_matrix, _ = cv2.Rodrigues(rvec)
        z_axis = np.array([0, 0, 1])
        camera_z = rotation_matrix @ z_axis
        angle_with_z = np.arccos(np.clip(np.abs(np.dot(camera_z, z_axis)), 0, 1))
        angle_degrees = np.degrees(angle_with_z)
        
        if angle_degrees > 75:
            issues.append(f"ê°ë„ ë„ˆë¬´ ê·¹ë‹¨ì  ({angle_degrees:.1f}ë„)")
            quality_score -= 0.1
        else:
            angle_score = 1.0 - (angle_degrees / 75.0)
            quality_score += angle_score * 0.2
        
        # 4. ì½”ë„ˆ ë¶„í¬ ê²€ì‚¬ (ì•ˆì „í•œ ì²˜ë¦¬)
        if charuco_corners is not None and corner_count > 0:
            try:
                corners_array = np.array(charuco_corners).reshape(-1, 2)
                std_x = np.std(corners_array[:, 0])
                std_y = np.std(corners_array[:, 1])
                
                if std_x < 30 or std_y < 30:
                    issues.append("ì½”ë„ˆ ë¶„í¬ ì§‘ì¤‘ë¨")
                    quality_score -= 0.05
            except Exception as e:
                print(f"\rDEBUG: Corner distribution calculation failed: {e}", end="")
        
        # 5. ê¸°ë³¸ ì ìˆ˜
        quality_score += 0.3
        print(f"\rDEBUG: Base score added, final score: {quality_score:.2f}", end="")
        
        # ìµœì¢… í’ˆì§ˆ í‰ê°€
        is_good_quality = quality_score >= self.min_detection_confidence
        
        if is_good_quality:
            message = f"í’ˆì§ˆ ì–‘í˜¸ (ì ìˆ˜: {quality_score:.2f})"
        else:
            message = f"í’ˆì§ˆ ë¶ˆëŸ‰ (ì ìˆ˜: {quality_score:.2f}) - {'; '.join(issues)}"
        
        print(f"\rDEBUG: Final quality: {quality_score:.2f}, threshold: {self.min_detection_confidence}, good: {is_good_quality}", end="")
        return is_good_quality, message, quality_score
    
    def _euler_to_rotation_matrix(self, rx: float, ry: float, rz: float) -> np.ndarray:
        """
        ì˜¤ì¼ëŸ¬ ê°ë„(RPY)ë¥¼ íšŒì „ í–‰ë ¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        Args:
            rx: Xì¶• íšŒì „ (ë¼ë””ì•ˆ)
            ry: Yì¶• íšŒì „ (ë¼ë””ì•ˆ)
            rz: Zì¶• íšŒì „ (ë¼ë””ì•ˆ)
            
        Returns:
            np.ndarray: 3x3 íšŒì „ í–‰ë ¬
        """
        # ê° ì¶•ì— ëŒ€í•œ íšŒì „ í–‰ë ¬
        Rx = np.array([
            [1, 0, 0],
            [0, np.cos(rx), -np.sin(rx)],
            [0, np.sin(rx), np.cos(rx)]
        ])
        
        Ry = np.array([
            [np.cos(ry), 0, np.sin(ry)],
            [0, 1, 0],
            [-np.sin(ry), 0, np.cos(ry)]
        ])
        
        Rz = np.array([
            [np.cos(rz), -np.sin(rz), 0],
            [np.sin(rz), np.cos(rz), 0],
            [0, 0, 1]
        ])
        
        # ZYX ìˆœì„œë¡œ íšŒì „ í–‰ë ¬ ê³±ì…ˆ
        return Rz @ Ry @ Rx
    
    
    def collect_angles_realtime_simple(self, num_poses: int = 20) -> bool:
        """
        ê°„ë‹¨í•œ ì‹¤ì‹œê°„ ê°ë„ ìˆ˜ì§‘ ëª¨ë“œ (ë” ì•ˆì •ì ì¸ ë²„ì „)
        
        Args:
            num_poses: ìˆ˜ì§‘í•  í¬ì¦ˆ ê°œìˆ˜
            
        Returns:
            bool: ìˆ˜ì§‘ ì„±ê³µ ì—¬ë¶€
        """
        print(f"\n=== ì‹¤ì‹œê°„ ê°ë„ ìˆ˜ì§‘ ëª¨ë“œ (ê°„ë‹¨ ë²„ì „) ===")
        print(f"ìˆ˜ì§‘í•  í¬ì¦ˆ ìˆ˜: {num_poses}")
        print("ë¡œë´‡ì„ ìˆ˜ë™ìœ¼ë¡œ ì¡°ì‘í•˜ì—¬ ë‹¤ì–‘í•œ í¬ì¦ˆì˜ ê°ë„ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
        print("ê° í¬ì¦ˆì—ì„œ Enterë¥¼ ëˆŒëŸ¬ ì €ì¥í•˜ê±°ë‚˜ 'q'ë¥¼ ëˆŒëŸ¬ ì¢…ë£Œí•˜ì„¸ìš”.")
        
        collected_poses = 0
        pose_index = 0
        
        while collected_poses < num_poses:
            try:
                # í˜„ì¬ ê°ë„ ê°€ì ¸ì˜¤ê¸°
                angles = self.get_robot_angles()
                coords = self.robot.get_coords()
                
                # í˜„ì¬ ìƒíƒœ ì¶œë ¥
                print(f"\n{'='*60}")
                print(f"í˜„ì¬ ìƒíƒœ: {collected_poses}/{num_poses} í¬ì¦ˆ ìˆ˜ì§‘ë¨")
                print(f"ëª©í‘œ: {num_poses}ê°œ í¬ì¦ˆ")
                print(f"í˜„ì¬ í¬ì¦ˆ ì¸ë±ìŠ¤: {pose_index}")
                print(f"\në¡œë´‡ ìƒíƒœ:")
                print(f"   ê°ë„: [{angles[0]:6.1f}, {angles[1]:6.1f}, {angles[2]:6.1f}, "
                      f"{angles[3]:6.1f}, {angles[4]:6.1f}, {angles[5]:6.1f}]")
                print(f"   ì¢Œí‘œ: [{coords[0]:6.1f}, {coords[1]:6.1f}, {coords[2]:6.1f}]")
                print(f"\nâŒ¨ì¡°ì‘ë²•:")
                print(f"   Enter: í˜„ì¬ í¬ì¦ˆ ì €ì¥")
                print(f"   q: ì¢…ë£Œ")
                print(f"   Ctrl+C: ê°•ì œ ì¢…ë£Œ")
                print(f"{'='*60}")
                
                # ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸°
                user_input = input("\nëª…ë ¹ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip().lower()
                
                if user_input == 'q':
                    print(f"\nğŸ›‘ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. ìˆ˜ì§‘ëœ í¬ì¦ˆ: {collected_poses}")
                    break
                elif user_input == '' or user_input == 's':
                    # í˜„ì¬ í¬ì¦ˆ ì €ì¥
                    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
                    
                    angle_data = {
                        "angles": angles,
                        "coords": coords,
                        "pose_index": pose_index,
                        "timestamp": timestamp
                    }
                    
                    angle_path = self.angles_dir / f"{pose_index:02d}_{timestamp}_angles.json"
                    with open(angle_path, 'w') as f:
                        json.dump(angle_data, f, indent=2)
                    
                    print(f"\nâœ… í¬ì¦ˆ {pose_index} ì €ì¥ ì™„ë£Œ!")
                    print(f"   ğŸ“ ì €ì¥ ìœ„ì¹˜: {angle_path}")
                    collected_poses += 1
                    pose_index += 1
                    
                    if collected_poses < num_poses:
                        print(f"\nğŸ‰ {collected_poses}/{num_poses} í¬ì¦ˆ ìˆ˜ì§‘ ì™„ë£Œ!")
                        input("ë‹¤ìŒ í¬ì¦ˆë¡œ ê³„ì†í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
                    else:
                        print(f"\nğŸ‰ ëª¨ë“  í¬ì¦ˆ ìˆ˜ì§‘ ì™„ë£Œ!")
                
            except KeyboardInterrupt:
                print(f"\nğŸ›‘ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. ìˆ˜ì§‘ëœ í¬ì¦ˆ: {collected_poses}")
                break
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                input("ê³„ì†í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
                continue
        
        print(f"\n{'='*60}")
        print(f"ì‹¤ì‹œê°„ ê°ë„ ìˆ˜ì§‘ ì™„ë£Œ")
        print(f"ì´ {collected_poses}ê°œì˜ í¬ì¦ˆ ë°ì´í„°ê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"{'='*60}")
        
        return collected_poses >= 10  # ìµœì†Œ 10ê°œ ì´ìƒ í•„ìš”
    
    def collect_angles_with_camera_feed(self, num_poses: int = 20) -> bool:
        """
        ì¹´ë©”ë¼ í™”ë©´ì„ ë³´ì—¬ì£¼ë©´ì„œ ì‹¤ì‹œê°„ ê°ë„ ìˆ˜ì§‘ ëª¨ë“œ (ì¹´ë©”ë¼ ìº¡ì²˜ í¬í•¨)
        
        Args:
            num_poses: ìˆ˜ì§‘í•  í¬ì¦ˆ ê°œìˆ˜
            
        Returns:
            bool: ìˆ˜ì§‘ ì„±ê³µ ì—¬ë¶€
        """
        print(f"\n=== ì‹¤ì‹œê°„ ê°ë„ ìˆ˜ì§‘ ëª¨ë“œ (ì¹´ë©”ë¼ í™”ë©´ í¬í•¨) ===")
        print(f"ìˆ˜ì§‘í•  í¬ì¦ˆ ìˆ˜: {num_poses}")
        print("ë¡œë´‡ì„ ìˆ˜ë™ìœ¼ë¡œ ì¡°ì‘í•˜ì—¬ ë‹¤ì–‘í•œ í¬ì¦ˆì˜ ê°ë„ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
        print("ì¹´ë©”ë¼ í™”ë©´ì´ ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œë˜ë©°, ChArUco ê²€ì¶œ ìƒíƒœë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("ê° í¬ì¦ˆì—ì„œ 's'ë¥¼ ëˆŒëŸ¬ ì €ì¥í•˜ê±°ë‚˜ 'q'ë¥¼ ëˆŒëŸ¬ ì¢…ë£Œí•˜ì„¸ìš”.")
        print("ì €ì¥ ì‹œ color ì´ë¯¸ì§€, depth, intrinsicsë„ í•¨ê»˜ ì €ì¥ë©ë‹ˆë‹¤.")
        
        # RealSense ì¹´ë©”ë¼ ì´ˆê¸°í™” (848x480 í•´ìƒë„)
        config = rs.config()
        config.enable_stream(rs.stream.color, 848, 480, rs.format.bgr8, 30)  
        config.enable_stream(rs.stream.depth, 848, 480, rs.format.z16, 30)
        
        pipeline = None
        try:
            pipeline = rs.pipeline()
            profile = pipeline.start(config)
            print("âœ… RealSense ì¹´ë©”ë¼ ì—°ê²° ì„±ê³µ (848x480)")
        except Exception as e:
            print(f"âŒ ì¹´ë©”ë¼ ì—°ê²° ì‹¤íŒ¨: {e}")
            print("ì¹´ë©”ë¼ ì—†ì´ ê°ë„ ìˆ˜ì§‘ì„ ê³„ì†í•©ë‹ˆë‹¤...")
            return self.collect_angles_realtime_simple(num_poses)
        
        # RealSenseì—ì„œ ì‹¤ì œ ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
        color_profile = profile.get_stream(rs.stream.color)
        color_intrinsics = color_profile.as_video_stream_profile().get_intrinsics()
        
        # ì¹´ë©”ë¼ í–‰ë ¬ êµ¬ì„±
        camera_matrix = np.array([
            [color_intrinsics.fx, 0.0, color_intrinsics.ppx],
            [0.0, color_intrinsics.fy, color_intrinsics.ppy],
            [0.0, 0.0, 1.0]
        ], dtype=np.float64)
        
        # ì™œê³¡ ê³„ìˆ˜
        dist_coeffs = np.array(color_intrinsics.coeffs, dtype=np.float64)
        
        print(f"âœ… ì‹¤ì œ ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„° ë¡œë“œ ì™„ë£Œ:")
        print(f"  - í•´ìƒë„: {color_intrinsics.width}x{color_intrinsics.height}")
        print(f"  - fx: {color_intrinsics.fx:.3f}, fy: {color_intrinsics.fy:.3f}")
        print(f"  - ì£¼ì : ({color_intrinsics.ppx:.3f}, {color_intrinsics.ppy:.3f})")
        print(f"  - ì™œê³¡ ëª¨ë¸: {color_intrinsics.model}")
        print(f"  - ì™œê³¡ ê³„ìˆ˜: {color_intrinsics.coeffs}")
        
        collected_poses = 0
        pose_index = 0
        
        try:
            while collected_poses < num_poses:
                self.robot.release_all_servos()
                # ì¹´ë©”ë¼ í”„ë ˆì„ ê°€ì ¸ì˜¤ê¸°
                frames = pipeline.wait_for_frames()
                color_frame = frames.get_color_frame()
                depth_frame = frames.get_depth_frame()
                
                if not color_frame:
                    continue
                
                # ì´ë¯¸ì§€ ë³€í™˜
                color_image = np.asanyarray(color_frame.get_data())
                
                # Depth ì´ë¯¸ì§€ ë³€í™˜ (ì €ì¥ìš©)
                depth_image = None
                if depth_frame:
                    depth_image = np.asanyarray(depth_frame.get_data())
                
                # ChArUco ê²€ì¶œ
                charuco_detected = False
                detection_info = "ê²€ì¶œ ì•ˆë¨"
                charuco_pose = None
                quality_info = ""
                quality_score = 0.0
                charuco_result = None  # ì´ˆê¸°í™” ì¶”ê°€
                distance = 0.0  # ì´ˆê¸°í™” ì¶”ê°€
                
                try:
                    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ë¡œ ë§ˆì»¤ ê²€ì¶œ ì •í™•ë„ í–¥ìƒ
                    # 1. Grayscale ë³€í™˜
                    gray = cv2.cvtColor(color_image, cv2.COLOR_BGR2GRAY)
                    
                    # 2. ëŒ€ë¹„ í–¥ìƒ (CLAHE - Contrast Limited Adaptive Histogram Equalization)
                    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
                    gray_enhanced = clahe.apply(gray)
                    
                    # 3. ë…¸ì´ì¦ˆ ì œê±° (ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬) - ì œê±°
                    # gray_enhanced = cv2.GaussianBlur(gray_enhanced, (3, 3), 0)
                    
                    # 4. ì´ì§„í™” (Otsu's method)
                    _, binary = cv2.threshold(gray_enhanced, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
                    
                    # ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ë¥¼ ì»¬ëŸ¬ë¡œ ë³€í™˜ (ì—¬ê¸°ì„œ ë¯¸ë¦¬ ìƒì„±)
                    gray_enhanced_colored = cv2.cvtColor(gray_enhanced, cv2.COLOR_GRAY2BGR)
                    
                    try:
                        aruco_params = cv2.aruco.DetectorParameters()
                        # ê²€ì¶œ íŒŒë¼ë¯¸í„° ìµœì í™”
                        aruco_params.adaptiveThreshWinSizeMin = 3
                        aruco_params.adaptiveThreshWinSizeMax = 23
                        aruco_params.adaptiveThreshWinSizeStep = 10
                        aruco_params.adaptiveThreshConstant = 7
                        aruco_params.minMarkerPerimeterRate = 0.03
                        aruco_params.maxMarkerPerimeterRate = 4.0
                        aruco_params.polygonalApproxAccuracyRate = 0.03
                        aruco_params.minCornerDistanceRate = 0.05
                        aruco_params.minMarkerDistanceRate = 0.05
                        aruco_params.minDistanceToBorder = 3
                        aruco_params.minOtsuStdDev = 5.0
                        aruco_params.perspectiveRemovePixelPerCell = 4
                        aruco_params.perspectiveRemoveIgnoredMarginPerCell = 0.13
                        aruco_params.maxErroneousBitsInBorderRate = 0.35
                        aruco_params.errorCorrectionRate = 0.6
                        
                        aruco_detector = cv2.aruco.ArucoDetector(self.aruco_dict, aruco_params)
                        corners, ids, rejected = aruco_detector.detectMarkers(gray_enhanced)
                    except AttributeError:
                        # OpenCV 4.4 ì´í•˜ ë²„ì „
                        aruco_params = cv2.aruco.DetectorParameters_create()
                        # ê²€ì¶œ íŒŒë¼ë¯¸í„° ìµœì í™”
                        aruco_params.adaptiveThreshWinSizeMin = 3
                        aruco_params.adaptiveThreshWinSizeMax = 23
                        aruco_params.adaptiveThreshWinSizeStep = 10
                        aruco_params.adaptiveThreshConstant = 7
                        aruco_params.minMarkerPerimeterRate = 0.03
                        aruco_params.maxMarkerPerimeterRate = 4.0
                        aruco_params.polygonalApproxAccuracyRate = 0.03
                        aruco_params.minCornerDistanceRate = 0.05
                        aruco_params.minMarkerDistanceRate = 0.05
                        aruco_params.minDistanceToBorder = 3
                        aruco_params.minOtsuStdDev = 5.0
                        aruco_params.perspectiveRemovePixelPerCell = 4
                        aruco_params.perspectiveRemoveIgnoredMarginPerCell = 0.13
                        aruco_params.maxErroneousBitsInBorderRate = 0.35
                        aruco_params.errorCorrectionRate = 0.6
                        
                        corners, ids, rejected = cv2.aruco.detectMarkers(gray_enhanced, self.aruco_dict, parameters=aruco_params)
                    
                    # ê²€ì¶œëœ ë§ˆì»¤ ê·¸ë¦¬ê¸° (ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ì—)
                    if ids is not None and len(ids) > 0:
                        cv2.aruco.drawDetectedMarkers(gray_enhanced_colored, corners, ids)
                        # ArUco ë§ˆì»¤ ê°œìˆ˜ ì €ì¥
                        self._last_marker_count = len(ids)
                        print(f"\rDEBUG: ArUco markers detected: {len(ids)}", end="")
                    else:
                        self._last_marker_count = 0
                    
                    # ChArUco í¬ì¦ˆ ê²€ì¶œ
                    charuco_result = self.detect_charuco_pose(color_image, camera_matrix, dist_coeffs)
                    if charuco_result is not None:
                        rvec, tvec, charuco_corners, charuco_ids = charuco_result
                        charuco_detected = True
                        distance = np.linalg.norm(tvec)
                        detection_info = f"ê²€ì¶œë¨ (ê±°ë¦¬: {distance*1000:.1f}mm)"
                        
                        # í’ˆì§ˆ í‰ê°€
                        if charuco_corners is not None and charuco_ids is not None:
                            print(f"\rDEBUG: ChArUco corners={len(charuco_corners)}, ids={len(charuco_ids)}, min_required={self.min_charuco_corners}", end="")
                            
                            is_good_quality, quality_message, quality_score = self.evaluate_charuco_quality(
                                charuco_corners, charuco_ids, rvec, tvec
                            )
                            quality_info = f"Quality: {quality_score:.2f} ({'GOOD' if is_good_quality else 'LOW'})"
                            
                            # ë””ë²„ê·¸ ì •ë³´ ì¶œë ¥ (ì½˜ì†”)
                            print(f"\rChArUco: {len(charuco_corners)} corners, Distance: {distance*1000:.1f}mm, Quality: {quality_score:.2f}", end="")
                            
                            # í’ˆì§ˆì´ ì¢‹ì€ ê²½ìš°ì—ë§Œ ì¢Œí‘œì¶• ê·¸ë¦¬ê¸° (ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ì—)
                            if is_good_quality:
                                cv2.drawFrameAxes(gray_enhanced_colored, camera_matrix, dist_coeffs, rvec, tvec, 0.05)
                        else:
                            # ArUco ë§ˆì»¤ë§Œ ê²€ì¶œëœ ê²½ìš°ì—ë„ í’ˆì§ˆ í‰ê°€ ì‹œë„
                            print(f"\rDEBUG: ArUco markers only, trying alternative quality assessment", end="")
                            
                            # ArUco ë§ˆì»¤ ê°œìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •êµí•œ í’ˆì§ˆ í‰ê°€
                            if hasattr(self, '_last_marker_count'):
                                marker_count = self._last_marker_count
                                if marker_count >= 15:  # ì¶©ë¶„í•œ ArUco ë§ˆì»¤ê°€ ê²€ì¶œëœ ê²½ìš°
                                    quality_score = 0.5  # ë” ë†’ì€ ì ìˆ˜
                                    is_good_quality = quality_score >= self.min_detection_confidence
                                    quality_info = f"Quality: {quality_score:.2f} (ArUco: {marker_count}) ({'GOOD' if is_good_quality else 'LOW'})"
                                    print(f"\rArUco: {marker_count} markers, Quality: {quality_score:.2f}", end="")
                                elif marker_count >= 10:  # ì¤‘ê°„ ìˆ˜ì¤€ì˜ ArUco ë§ˆì»¤
                                    quality_score = 0.3
                                    is_good_quality = quality_score >= self.min_detection_confidence
                                    quality_info = f"Quality: {quality_score:.2f} (ArUco: {marker_count}) ({'GOOD' if is_good_quality else 'LOW'})"
                                    print(f"\rArUco: {marker_count} markers, Quality: {quality_score:.2f}", end="")
                                else:
                                    quality_score = 0.1
                                    is_good_quality = False
                                    quality_info = f"Quality: {quality_score:.2f} (ArUco: {marker_count}) (LOW)"
                                    print(f"\rArUco: {marker_count} markers (insufficient)", end="")
                            else:
                                quality_score = 0.0
                                is_good_quality = False
                                quality_info = "Quality: N/A (ArUco only)"
                                print(f"\rChArUco: ArUco markers only", end="")
                        
                except Exception as e:
                    detection_info = f"ê²€ì¶œ ì˜¤ë¥˜: {str(e)[:20]}"
                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì´ë¯¸ì§€ ì‚¬ìš©
                    gray_enhanced_colored = color_image.copy()
                
                # í˜„ì¬ ë¡œë´‡ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
                try:
                    angles = self.get_robot_angles()
                    coords = self.robot.get_coords()
                except Exception as e:
                    angles = [0, 0, 0, 0, 0, 0]
                    coords = [0, 0, 0, 0, 0, 0]
                    # ì‹¤ì‹œê°„ ëª¨ë“œì—ì„œëŠ” ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶œë ¥í•˜ì§€ ì•ŠìŒ
                
                # í™”ë©´ì— ì •ë³´ í‘œì‹œ (ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ì‚¬ìš©)
                # gray_enhanced_coloredëŠ” ì´ë¯¸ ìœ„ì—ì„œ ìƒì„±ë¨
                info_image = gray_enhanced_colored.copy()
                
                # ìƒíƒœ ì •ë³´ í…ìŠ¤íŠ¸ (ì˜ì–´ë¡œ í‘œì‹œ)
                cv2.putText(info_image, f"Pose: {collected_poses}/{num_poses}", (10, 30), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                
                # ì „ì²˜ë¦¬ ì •ë³´ í‘œì‹œ
                cv2.putText(info_image, "Enhanced Image (CLAHE only)", (10, 225), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 0), 1)
                
                # ChArUco ì½”ë„ˆ ê²€ì¶œ ìƒíƒœ í‘œì‹œ
                if charuco_result is not None:
                    rvec, tvec, charuco_corners, charuco_ids = charuco_result
                    corner_count = len(charuco_corners) if charuco_corners is not None else 0
                    id_count = len(charuco_ids) if charuco_ids is not None else 0
                    cv2.putText(info_image, f"ChArUco Corners: {corner_count}, IDs: {id_count}", (10, 205), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 255), 1)
                else:
                    cv2.putText(info_image, "ChArUco Corners: NOT DETECTED", (10, 205), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 255), 1)
                
                # ChArUco ê²€ì¶œ ìƒíƒœ í‘œì‹œ
                if charuco_detected:
                    cv2.putText(info_image, f"ChArUco: DETECTED ({distance*1000:.1f}mm)", (10, 60), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                else:
                    cv2.putText(info_image, "ChArUco: NOT DETECTED", (10, 60), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
                
                # í’ˆì§ˆ ì •ë³´ í‘œì‹œ
                if quality_info:
                    quality_color = (0, 255, 0) if quality_score >= self.min_detection_confidence else (0, 0, 255)
                    cv2.putText(info_image, f"Quality: {quality_score:.2f} ({'GOOD' if quality_score >= self.min_detection_confidence else 'LOW'})", (10, 110), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, quality_color, 1)
                
                cv2.putText(info_image, f"Angles: [{angles[0]:.1f}, {angles[1]:.1f}, {angles[2]:.1f}]", (10, 135),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                cv2.putText(info_image, f"        [{angles[3]:.1f}, {angles[4]:.1f}, {angles[5]:.1f}]", (10, 155), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                cv2.putText(info_image, f"Coords: [{coords[0]:.1f}, {coords[1]:.1f}, {coords[2]:.1f}]", (10, 180), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                
                # ì €ì¥ ê°€ëŠ¥ ì—¬ë¶€ í‘œì‹œ
                if charuco_detected:
                    corner_count = len(charuco_corners) if charuco_corners is not None else 0
                    if corner_count >= 6:
                        cv2.putText(info_image, "Press 's' to save", (10, 250), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
                    else:
                        cv2.putText(info_image, f"Need >=6 corners (current: {corner_count})", (10, 250), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 165, 255), 2)
                else:
                    cv2.putText(info_image, "No ChArUco detected - cannot save", (10, 250), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
                
                # í™”ë©´ í‘œì‹œ
                cv2.imshow('Eye-to-Hand Calibration - Real-time', info_image)
                
                # í‚¤ë³´ë“œ ì…ë ¥ í™•ì¸
                key = cv2.waitKey(1) & 0xFF
                
                if key == ord('q'):
                    print(f"\n ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. ìˆ˜ì§‘ëœ í¬ì¦ˆ: {collected_poses}")
                    break
                elif key == ord('p'):
                    # ë¡œë´‡ ê³ ì • ë° ì•ˆì •í™” ëŒ€ê¸°
                    print(f"\nğŸ”’ ë¡œë´‡ì„ ê³ ì •í•˜ê³  ì•ˆì •í™” ëŒ€ê¸° ì¤‘... (1ì´ˆ)")
                    self.robot.power_on()
                    time.sleep(1.0)  # 1ì´ˆ ëŒ€ê¸°ë¡œ ë¡œë´‡ ì•ˆì •í™”
                    
                    # ì•ˆì •í™” í›„ ë‹¤ì‹œ ì¹´ë©”ë¼ í”„ë ˆì„ê³¼ ChArUco ê²€ì¶œ
                    frames = pipeline.wait_for_frames()
                    color_frame = frames.get_color_frame()
                    if color_frame:
                        color_image = np.asanyarray(color_frame.get_data())
                        charuco_result = self.detect_charuco_pose(color_image, camera_matrix, dist_coeffs)
                        
                        if charuco_result is not None:
                            rvec, tvec, charuco_corners, charuco_ids = charuco_result
                            distance = np.linalg.norm(tvec)
                            
                            # ìµœì†Œ ì½”ë„ˆ ê°œìˆ˜ ì²´í¬ (6ê°œ ì´ìƒ í•„ìš”)
                            corner_count = len(charuco_corners) if charuco_corners is not None else 0
                            if corner_count < 6:
                                print(f"\nâš ï¸ ì €ì¥ ê±´ë„ˆëœ€: ì½”ë„ˆ ê°œìˆ˜ ë¶€ì¡± ({corner_count} < 6)")
                                print("ChArUco ë³´ë“œê°€ ë” ì˜ ë³´ì´ë„ë¡ ì¡°ì •í•´ì£¼ì„¸ìš”.")
                                continue
                            
                            # í’ˆì§ˆ í‰ê°€
                            if charuco_corners is not None and charuco_ids is not None:
                                is_good_quality, quality_message, quality_score = self.evaluate_charuco_quality(
                                    charuco_corners, charuco_ids, rvec, tvec
                                )
                            else:
                                # ArUco ë§ˆì»¤ ê¸°ë°˜ fallback ê²°ê³¼ì¸ ê²½ìš°
                                is_good_quality = True  # ArUco ë§ˆì»¤ ê¸°ë°˜ì€ ê¸°ë³¸ì ìœ¼ë¡œ ì–‘í˜¸ë¡œ ê°„ì£¼
                                quality_score = 0.5
                            
                            if is_good_quality:
                                # í˜„ì¬ ë¡œë´‡ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
                                angles = self.get_robot_angles()
                                coords = self.robot.get_coords()
                                
                                # ë°ì´í„° ì €ì¥
                                timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
                                
                                # ê°ë„ ë°ì´í„° ì €ì¥
                                angle_data = {
                                    "angles": angles,
                                    "coords": coords,
                                    "pose_index": pose_index,
                                    "timestamp": timestamp,
                                    "stabilization_wait": True
                                }
                                angle_path = self.angles_dir / f"{pose_index:02d}_{timestamp}_angles.json"
                                with open(angle_path, 'w') as f:
                                    json.dump(angle_data, f, indent=2)
                                
                                # í˜„ì¬ ë¡œë´‡ í¬ì¦ˆ ê°€ì ¸ì˜¤ê¸°
                                R_base2ee, t_base2ee = self.get_robot_pose()
                                
                                # ë¡œë´‡ í¬ì¦ˆ ì €ì¥
                                pose_data = {
                                    "R_base2ee": R_base2ee.tolist(),
                                    "t_base2ee": t_base2ee.tolist(),
                                    "angles": angles,
                                    "pose_index": pose_index,
                                    "timestamp": timestamp
                                }
                                pose_path = self.poses_dir / f"{pose_index:02d}_{timestamp}_pose.json"
                                with open(pose_path, 'w') as f:
                                    json.dump(pose_data, f, indent=2)
                                
                                # Color ì´ë¯¸ì§€ ì €ì¥
                                color_path = self.save_dir / "color" / f"{pose_index:02d}_{timestamp}.jpg"
                                color_path.parent.mkdir(parents=True, exist_ok=True)
                                cv2.imwrite(str(color_path), color_image)
                                
                                # Depth ì´ë¯¸ì§€ ì €ì¥ (numpy ë°°ì—´ë¡œ)
                                if depth_image is not None:
                                    depth_path = self.save_dir / "depth" / f"{pose_index:02d}_{timestamp}.npy"
                                    depth_path.parent.mkdir(parents=True, exist_ok=True)
                                    np.save(str(depth_path), depth_image)
                                    
                                    # Depth ì´ë¯¸ì§€ë¥¼ PNGë¡œë„ ì €ì¥ (ì‹œê°í™”ìš©)
                                    depth_png_path = self.save_dir / "depth" / "converted_png" / f"{pose_index:02d}_{timestamp}.png"
                                    depth_png_path.parent.mkdir(parents=True, exist_ok=True)
                                    # Depth ì´ë¯¸ì§€ë¥¼ 0-255 ë²”ìœ„ë¡œ ì •ê·œí™”
                                    depth_normalized = cv2.normalize(depth_image, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)
                                    cv2.imwrite(str(depth_png_path), depth_normalized)
                                
                                # Intrinsics ì €ì¥ (ì‹¤ì œ ì¹´ë©”ë¼ íŒŒë¼ë¯¸í„° ì‚¬ìš©)
                                intrinsics_data = {
                                    "color_intrinsics": {
                                        "width": color_intrinsics.width,
                                        "height": color_intrinsics.height,
                                        "fx": float(color_intrinsics.fx),  # ì´ˆì  ê±°ë¦¬ x
                                        "fy": float(color_intrinsics.fy),  # ì´ˆì  ê±°ë¦¬ y
                                        "ppx": float(color_intrinsics.ppx),  # ì£¼ì  x ì¢Œí‘œ
                                        "ppy": float(color_intrinsics.ppy),  # ì£¼ì  y ì¢Œí‘œ
                                        "distortion_model": str(color_intrinsics.model),  # enumì„ ë¬¸ìì—´ë¡œ ë³€í™˜
                                        "distortion_coeffs": [float(coeff) for coeff in color_intrinsics.coeffs]  # ë Œì¦ˆ ì™œê³¡ ê³„ìˆ˜
                                    },
                                    "camera_matrix": camera_matrix.tolist(),
                                    "dist_coeffs": dist_coeffs.tolist(),
                                    "timestamp": datetime.now().isoformat()
                                }
                                intrinsics_path = self.save_dir / "intrinsics" / f"{pose_index:02d}_{timestamp}.json"
                                intrinsics_path.parent.mkdir(parents=True, exist_ok=True)
                                with open(intrinsics_path, 'w') as f:
                                    json.dump(intrinsics_data, f, indent=2)
                                
                                # ChArUco ë°ì´í„° ì €ì¥ (íƒ€ê²Ÿ -> ì¹´ë©”ë¼) - Eye-to-Hand êµ¬ì¡°
                                # detect_charuco_pose()ëŠ” ì¹´ë©”ë¼ -> íƒ€ê²Ÿì„ ë°˜í™˜í•˜ë¯€ë¡œ ì—­í–‰ë ¬ì„ ì·¨í•¨
                                R_cam2target, _ = cv2.Rodrigues(rvec)
                                t_cam2target = tvec
                                
                                # íƒ€ê²Ÿ -> ì¹´ë©”ë¼ ë³€í™˜ (ì—­í–‰ë ¬)
                                R_target2cam = R_cam2target.T
                                t_target2cam = -R_target2cam @ t_cam2target
                                
                                charuco_data = {
                                    "rvec_target2cam": cv2.Rodrigues(R_target2cam)[0].tolist(),
                                    "tvec_target2cam": t_target2cam.tolist(),
                                    "rvec_cam2target": rvec.tolist(),  # ì›ë³¸ ë°ì´í„°ë„ ë³´ì¡´
                                    "tvec_cam2target": tvec.tolist(),
                                    "pose_index": pose_index,
                                    "timestamp": timestamp,
                                    "charuco_corners_count": len(charuco_corners) if charuco_corners is not None else 0,
                                    "charuco_ids_count": len(charuco_ids) if charuco_ids is not None else 0,
                                    "distance_mm": float(distance * 1000),
                                    "quality_score": float(quality_score),
                                    "detection_method": "charuco_corners" if charuco_corners is not None else "aruco_markers",
                                    "color_image_path": str(color_path),
                                    "depth_image_path": str(depth_path) if depth_image is not None else None,
                                    "intrinsics_path": str(intrinsics_path)
                                }
                                charuco_path = self.save_dir / f"{pose_index:02d}_{timestamp}_charuco.json"
                                with open(charuco_path, 'w') as f:
                                    json.dump(charuco_data, f, indent=2)
                                
                                print(f"\nâœ… í¬ì¦ˆ {pose_index} ì €ì¥ ì™„ë£Œ!")
                                print(f"  - ê±°ë¦¬: {distance*1000:.1f}mm")
                                print(f"  - í’ˆì§ˆ: {quality_score:.2f}")
                                print(f"  - ê²€ì¶œ ë°©ë²•: {'ChArUco ì½”ë„ˆ' if charuco_corners is not None else 'ArUco ë§ˆì»¤'}")
                                print(f"  - ê°ë„ ë°ì´í„°: {angle_path}")
                                print(f"  - ë¡œë´‡ í¬ì¦ˆ: {pose_path}")
                                print(f"  - ChArUco í¬ì¦ˆ: {charuco_path}")
                                print(f"  - Color ì´ë¯¸ì§€: {color_path}")
                                if depth_image is not None:
                                    print(f"  - Depth ì´ë¯¸ì§€: {depth_path}")
                                    print(f"  - Depth PNG: {depth_png_path}")
                                print(f"  - Intrinsics: {intrinsics_path}")
                                
                                collected_poses += 1
                                pose_index += 1
                            else:
                                print(f"\nâŒ í’ˆì§ˆì´ ë‚®ì•„ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (í’ˆì§ˆ ì ìˆ˜: {quality_score:.2f})")
                        else:
                            print(f"\nâŒ ChArUco ê²€ì¶œ ì‹¤íŒ¨ë¡œ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    
        except KeyboardInterrupt:
            print(f"\n ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. ìˆ˜ì§‘ëœ í¬ì¦ˆ: {collected_poses}")
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        finally:
            # ì¹´ë©”ë¼ ì •ë¦¬
            if pipeline is not None:
                try:
                    pipeline.stop()
                except:
                    pass
            cv2.destroyAllWindows()
        
        print(f"\n{'='*60}")
        print(f" ì‹¤ì‹œê°„ ê°ë„ ìˆ˜ì§‘ ì™„ë£Œ")
        print(f" ì´ {collected_poses}ê°œì˜ í¬ì¦ˆ ë°ì´í„°ê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"{'='*60}")
        
        return collected_poses >= 10  # ìµœì†Œ 10ê°œ ì´ìƒ í•„ìš”
    
    def capture_data_at_angles(self, angle_data: Dict[str, Any], pose_index: int) -> bool:
        """
        ì €ì¥ëœ ê°ë„ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œë´‡ì„ ì´ë™ì‹œí‚¤ê³  ì´ë¯¸ì§€ë¥¼ ìº¡ì²˜í•©ë‹ˆë‹¤.
        
        Args:
            angle_data: ê°ë„ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            pose_index: í¬ì¦ˆ ì¸ë±ìŠ¤
            
        Returns:
            bool: ìº¡ì²˜ ì„±ê³µ ì—¬ë¶€
        """
        try:
            print(f"\n[{pose_index}] ìë™ ì´ë™ ëª¨ë“œ")
            
            # ì €ì¥ëœ ê°ë„ë¡œ ë¡œë´‡ ì´ë™
            angles = angle_data["angles"]
            print(f"ë¡œë´‡ì„ ê°ë„ {angles}ë¡œ ì´ë™ ì¤‘...")
            
            # ë¡œë´‡ ì´ë™ (ì†ë„ 50)
            self.robot.send_angles(angles, 50)
            time.sleep(3)  # ì´ë™ ì™„ë£Œ ëŒ€ê¸°
            
            # í˜„ì¬ ë¡œë´‡ í¬ì¦ˆ ê°€ì ¸ì˜¤ê¸°
            R_base2ee, t_base2ee = self.get_robot_pose()
            
            # ì´ë¯¸ì§€ ìº¡ì²˜
            print("ì´ë¯¸ì§€ ìº¡ì²˜ ì¤‘...")
            color_path, depth_path, intrinsics_path = capture_d455_images(
                save_dir=str(self.save_dir),
                rgb_size=(848, 480),
                depth_size=(848, 480)
            )
            
            # ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„° ë¡œë“œ
            camera_matrix, dist_coeffs = self._load_camera_intrinsics(intrinsics_path)
            
            # ì´ë¯¸ì§€ì—ì„œ ChArUco ê²€ì¶œ
            image = cv2.imread(color_path)
            charuco_pose = self.detect_charuco_pose(image, camera_matrix, dist_coeffs)
            
            if charuco_pose is None:
                print(f"[{pose_index}] ChArUco ê²€ì¶œ ì‹¤íŒ¨")
                print("ChArUco ë³´ë“œê°€ ì¹´ë©”ë¼ì— ì˜ ë³´ì´ëŠ”ì§€ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                return False
            
            rvec, tvec, charuco_corners, charuco_ids = charuco_pose
            
            # ê±°ë¦¬ ì •ë³´ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
            distance = np.linalg.norm(tvec)
            print(f"ChArUco ë³´ë“œê¹Œì§€ì˜ ê±°ë¦¬: {distance*1000:.1f}mm")
            
            # ë°ì´í„° ì €ì¥
            timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
            
            # ë¡œë´‡ í¬ì¦ˆ ì €ì¥
            pose_data = {
                "R_base2ee": R_base2ee.tolist(),
                "t_base2ee": t_base2ee.tolist(),
                "angles": angles,
                "pose_index": pose_index,
                "timestamp": timestamp
            }
            pose_path = self.poses_dir / f"{pose_index:02d}_{timestamp}_pose.json"
            
            with open(pose_path, 'w') as f:
                json.dump(pose_data, f, indent=2)
            
            # ChArUco í¬ì¦ˆ ì €ì¥ (íƒ€ê²Ÿ -> ì¹´ë©”ë¼) - Eye-to-Hand êµ¬ì¡°
            # detect_charuco_pose()ëŠ” ì¹´ë©”ë¼ -> íƒ€ê²Ÿì„ ë°˜í™˜í•˜ë¯€ë¡œ ì—­í–‰ë ¬ì„ ì·¨í•¨
            R_cam2target, _ = cv2.Rodrigues(rvec)
            t_cam2target = tvec
            
            # íƒ€ê²Ÿ -> ì¹´ë©”ë¼ ë³€í™˜ (ì—­í–‰ë ¬)
            R_target2cam = R_cam2target.T
            t_target2cam = -R_target2cam @ t_cam2target
            
            charuco_data = {
                "rvec_target2cam": cv2.Rodrigues(R_target2cam)[0].tolist(),
                "tvec_target2cam": t_target2cam.tolist(),
                "rvec_cam2target": rvec.tolist(),  # ì›ë³¸ ë°ì´í„°ë„ ë³´ì¡´
                "tvec_cam2target": tvec.tolist(),
                "pose_index": pose_index,
                "timestamp": timestamp
            }
            charuco_path = self.save_dir / f"{pose_index:02d}_{timestamp}_charuco.json"
            
            with open(charuco_path, 'w') as f:
                json.dump(charuco_data, f, indent=2)
            
            print(f"[{pose_index}] âœ… ì €ì¥ ì™„ë£Œ")
            print(f"  - ì´ë¯¸ì§€: {color_path}")
            print(f"  - ê¹Šì´: {depth_path}")
            print(f"  - ë¡œë´‡ í¬ì¦ˆ: {pose_path}")
            print(f"  - ChArUco í¬ì¦ˆ: {charuco_path}")
            
            return True
            
        except Exception as e:
            print(f"[{pose_index}] ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False
    
    def perform_automatic_calibration(self) -> bool:
        """
        ì €ì¥ëœ ê°ë„ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ìë™ìœ¼ë¡œ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Returns:
            bool: ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì„±ê³µ ì—¬ë¶€
        """
        print(f"\n=== ìë™ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ëª¨ë“œ ===")
        
        # ì €ì¥ëœ ê°ë„ íŒŒì¼ë“¤ ë¡œë“œ
        angle_files = list(self.angles_dir.glob("*_angles.json"))
        
        if len(angle_files) == 0:
            print("ì €ì¥ëœ ê°ë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì‹¤ì‹œê°„ ê°ë„ ìˆ˜ì§‘ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.")
            return False
        
        # íŒŒì¼ ì •ë ¬ (ì¸ë±ìŠ¤ ìˆœì„œëŒ€ë¡œ)
        angle_files.sort(key=lambda x: int(x.stem.split('_')[0]))
        
        print(f"ì´ {len(angle_files)}ê°œì˜ ê°ë„ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ìë™ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        successful_captures = 0
        
        for i, angle_file in enumerate(angle_files):
            print(f"\n[{i+1}/{len(angle_files)}] ê°ë„ ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
            
            # ê°ë„ ë°ì´í„° ë¡œë“œ
            with open(angle_file, 'r') as f:
                angle_data = json.load(f)
            
            pose_index = angle_data["pose_index"]
            
            if self.capture_data_at_angles(angle_data, pose_index):
                successful_captures += 1
                print(f"âœ… í¬ì¦ˆ {pose_index} ìº¡ì²˜ ì„±ê³µ ({successful_captures}/{len(angle_files)})")
            else:
                print(f"âŒ í¬ì¦ˆ {pose_index} ìº¡ì²˜ ì‹¤íŒ¨")
                
                # ì¬ì‹œë„ ì˜µì…˜
                retry = input("ì´ í¬ì¦ˆë¥¼ ë‹¤ì‹œ ì‹œë„í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower().strip()
                if retry == 'y':
                    i -= 1  # ê°™ì€ íŒŒì¼ì„ ë‹¤ì‹œ ì²˜ë¦¬
                    continue
        
        print(f"\n=== ìë™ ìº¡ì²˜ ì™„ë£Œ ===")
        print(f"ì„±ê³µ: {successful_captures}/{len(angle_files)}")
        
        if successful_captures >= 10:  # ìµœì†Œ 10ê°œ ì´ìƒì˜ í¬ì¦ˆê°€ í•„ìš”
            return self.calculate_transformation_matrix()
        else:
            print("ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆ˜ì§‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
    
    def capture_data_at_manual_pose(self, pose_index: int) -> bool:
        """
        ìˆ˜ë™ìœ¼ë¡œ ì¡°ì‘ëœ ë¡œë´‡ í¬ì¦ˆì—ì„œ ì´ë¯¸ì§€ì™€ í¬ì¦ˆ ë°ì´í„°ë¥¼ ìº¡ì²˜í•©ë‹ˆë‹¤.
        
        Args:
            pose_index: í¬ì¦ˆ ì¸ë±ìŠ¤
            
        Returns:
            bool: ìº¡ì²˜ ì„±ê³µ ì—¬ë¶€
        """
        try:
            print(f"\n[{pose_index}] ìˆ˜ë™ ì¡°ì‘ ëª¨ë“œ")
            
            # ë¡œë´‡ì„ ìˆ˜ë™ ì¡°ì‘ ëª¨ë“œë¡œ ì „í™˜
            self.robot.release_all_servos()
            
            # ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸°
            input("ë¡œë´‡ì„ ì›í•˜ëŠ” ìœ„ì¹˜ë¡œ ì´ë™í•œ í›„ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”...")
            
            # ë¡œë´‡ì„ ê³ ì • ëª¨ë“œë¡œ ì „í™˜
            self.robot.power_on()
            time.sleep(2)  # ë¡œë´‡ ì•ˆì •í™” ëŒ€ê¸°
            
            # í˜„ì¬ ë¡œë´‡ í¬ì¦ˆ ê°€ì ¸ì˜¤ê¸°
            R_base2ee, t_base2ee = self.get_robot_pose()
            
            # ì´ë¯¸ì§€ ìº¡ì²˜
            print("ì´ë¯¸ì§€ ìº¡ì²˜ ì¤‘...")
            color_path, depth_path, intrinsics_path = capture_d455_images(
                save_dir=str(self.save_dir),
                rgb_size=(848, 480),
                depth_size=(848, 480)
            )
            
            # ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„° ë¡œë“œ
            camera_matrix, dist_coeffs = self._load_camera_intrinsics(intrinsics_path)
            
            # ì´ë¯¸ì§€ì—ì„œ ChArUco ê²€ì¶œ
            image = cv2.imread(color_path)
            charuco_pose = self.detect_charuco_pose(image, camera_matrix, dist_coeffs)
            
            if charuco_pose is None:
                print(f"[{pose_index}] ChArUco ê²€ì¶œ ì‹¤íŒ¨")
                print("ChArUco ë³´ë“œê°€ ì¹´ë©”ë¼ì— ì˜ ë³´ì´ëŠ”ì§€ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                return False
            
            rvec, tvec, charuco_corners, charuco_ids = charuco_pose
            
            # ê±°ë¦¬ ì •ë³´ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
            distance = np.linalg.norm(tvec)
            print(f"ChArUco ë³´ë“œê¹Œì§€ì˜ ê±°ë¦¬: {distance*1000:.1f}mm")
            
            # ë°ì´í„° ì €ì¥
            timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
            
            # ë¡œë´‡ í¬ì¦ˆ ì €ì¥
            pose_data = {
                "R_base2ee": R_base2ee.tolist(),
                "t_base2ee": t_base2ee.tolist(),
                "pose_index": pose_index,
                "timestamp": timestamp
            }
            pose_path = self.poses_dir / f"{pose_index:02d}_{timestamp}_pose.json"
            
            with open(pose_path, 'w') as f:
                json.dump(pose_data, f, indent=2)
            
            # ChArUco í¬ì¦ˆ ì €ì¥ (íƒ€ê²Ÿ -> ì¹´ë©”ë¼) - Eye-to-Hand êµ¬ì¡°
            # detect_charuco_pose()ëŠ” ì¹´ë©”ë¼ -> íƒ€ê²Ÿì„ ë°˜í™˜í•˜ë¯€ë¡œ ì—­í–‰ë ¬ì„ ì·¨í•¨
            R_cam2target, _ = cv2.Rodrigues(rvec)
            t_cam2target = tvec
            
            # íƒ€ê²Ÿ -> ì¹´ë©”ë¼ ë³€í™˜ (ì—­í–‰ë ¬)
            R_target2cam = R_cam2target.T
            t_target2cam = -R_target2cam @ t_cam2target
            
            charuco_data = {
                "rvec_target2cam": cv2.Rodrigues(R_target2cam)[0].tolist(),
                "tvec_target2cam": t_target2cam.tolist(),
                "rvec_cam2target": rvec.tolist(),  # ì›ë³¸ ë°ì´í„°ë„ ë³´ì¡´
                "tvec_cam2target": tvec.tolist(),
                "pose_index": pose_index,
                "timestamp": timestamp
            }
            charuco_path = self.save_dir / f"{pose_index:02d}_{timestamp}_charuco.json"
            
            with open(charuco_path, 'w') as f:
                json.dump(charuco_data, f, indent=2)
            
            print(f"[{pose_index}] âœ… ì €ì¥ ì™„ë£Œ")
            print(f"  - ì´ë¯¸ì§€: {color_path}")
            print(f"  - ê¹Šì´: {depth_path}")
            print(f"  - ë¡œë´‡ í¬ì¦ˆ: {pose_path}")
            print(f"  - ChArUco í¬ì¦ˆ: {charuco_path}")
            
            return True
            
        except Exception as e:
            print(f"[{pose_index}] ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False
    
    def perform_manual_calibration(self, num_poses: int = 20) -> bool:
        """
        ìˆ˜ë™ ì¡°ì‘ì„ í†µí•œ Eye-to-Hand calibrationì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            num_poses: ìº¡ì²˜í•  í¬ì¦ˆ ê°œìˆ˜
            
        Returns:
            bool: ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì„±ê³µ ì—¬ë¶€
        """
        print(f"\n=== ìˆ˜ë™ Eye-in-Hand Calibration ì‹œì‘ ===")
        print(f"ìº¡ì²˜í•  í¬ì¦ˆ ìˆ˜: {num_poses}")
        print("ê° í¬ì¦ˆì—ì„œ ë¡œë´‡ì„ ìˆ˜ë™ìœ¼ë¡œ ì¡°ì‘í•˜ì—¬ ChArUco ë³´ë“œë¥¼ ë‹¤ì–‘í•œ ê°ë„ì—ì„œ ì´¬ì˜í•©ë‹ˆë‹¤.")
        
        successful_captures = 0
        current_pose_index = 0
        
        while successful_captures < num_poses:
            print(f"\n[{successful_captures}/{num_poses}] í¬ì¦ˆ ìº¡ì²˜ ì¤‘...")
            
            if self.capture_data_at_manual_pose(current_pose_index):
                successful_captures += 1
                print(f"âœ… í¬ì¦ˆ {successful_captures} ìº¡ì²˜ ì„±ê³µ ({successful_captures}/{num_poses})")
                # ê²€ì¶œ ì„±ê³µ ì‹œ ìë™ìœ¼ë¡œ ë‹¤ìŒ í¬ì¦ˆë¡œ ì§„í–‰
                current_pose_index += 1
            else:
                print(f"âŒ í¬ì¦ˆ {current_pose_index} ìº¡ì²˜ ì‹¤íŒ¨")
                
                # ì¬ì‹œë„ ì˜µì…˜ (ê²€ì¶œ ì‹¤íŒ¨ ì‹œì—ë§Œ ì§ˆë¬¸)
                retry = input("ì´ í¬ì¦ˆë¥¼ ë‹¤ì‹œ ì‹œë„í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n/q): ").lower().strip()
                if retry == 'q':
                    print("ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    break
                elif retry == 'y':
                    continue  # ê°™ì€ ì¸ë±ìŠ¤ë¡œ ë‹¤ì‹œ ì‹œë„
                else:
                    print("ì´ í¬ì¦ˆë¥¼ ê±´ë„ˆë›°ê³  ë‹¤ìŒìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                    current_pose_index += 1
        
        print(f"\n=== ìˆ˜ë™ ìº¡ì²˜ ì™„ë£Œ ===")
        print(f"ì„±ê³µ: {successful_captures}/{num_poses}")
        
        if successful_captures >= 10:  # ìµœì†Œ 10ê°œ ì´ìƒì˜ í¬ì¦ˆê°€ í•„ìš”
            return self.calculate_transformation_matrix()
        else:
            print("ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆ˜ì§‘ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
    
    def calculate_transformation_matrix(self) -> bool:
        """
        ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ Eye-to-Hand ë³€í™˜ í–‰ë ¬ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        """
        print("\n=== Eye-to-Hand ë³€í™˜ í–‰ë ¬ ê³„ì‚° ì¤‘ ===")
        
        try:
            # ì €ì¥ëœ ë°ì´í„° ë¡œë“œ
            pose_files = list(self.poses_dir.glob("*_pose.json"))
            charuco_files = list(self.save_dir.glob("*_charuco.json"))
            
            if len(charuco_files) < 3:
                print(f"ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (í•„ìš”: 3ê°œ ì´ìƒ, í˜„ì¬: {len(charuco_files)}ê°œ)")
                return False
            
            # ë°ì´í„° ì •ë ¬ (ì¸ë±ìŠ¤ ìˆœì„œëŒ€ë¡œ)
            pose_files.sort(key=lambda x: int(x.stem.split('_')[0]))
            charuco_files.sort(key=lambda x: int(x.stem.split('_')[0]))
            
            T_base2ee_list = []  # ë¡œë´‡ ë² ì´ìŠ¤ -> End-Effector (ë¯¸í„° ë‹¨ìœ„)
            T_cam2target_list = []  # ì¹´ë©”ë¼ -> ChArUco íƒ€ê²Ÿ (ë¯¸í„° ë‹¨ìœ„)
            quality_weights = []  # í’ˆì§ˆ ì ìˆ˜ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©
            
            print(f"ì´ {len(charuco_files)}ê°œì˜ í¬ì¦ˆ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
            
            # ëª¨ë“  charuco íŒŒì¼ì„ ì‚¬ìš© (ChArUco ì½”ë„ˆ ê¸°ë°˜ê³¼ ArUco ë§ˆì»¤ ê¸°ë°˜ ëª¨ë‘ í¬í•¨)
            valid_pose_files = []
            valid_charuco_files = []
            
            for charuco_file in charuco_files:
                # charuco íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ì‚¬ìš©
                valid_charuco_files.append(charuco_file)
                # í•´ë‹¹í•˜ëŠ” pose íŒŒì¼ ì°¾ê¸°
                pose_index = int(charuco_file.stem.split('_')[0])
                pose_file = self.poses_dir / f"{pose_index:02d}_*_pose.json"
                pose_files_found = list(self.poses_dir.glob(f"{pose_index:02d}_*_pose.json"))
                if pose_files_found:
                    valid_pose_files.append(pose_files_found[0])
                else:
                    # pose íŒŒì¼ì´ ì—†ìœ¼ë©´ angles íŒŒì¼ì—ì„œ ê°ë„ ì •ë³´ ì‚¬ìš©
                    angle_files_found = list(self.angles_dir.glob(f"{pose_index:02d}_*_angles.json"))
                    if angle_files_found:
                        with open(angle_files_found[0], 'r') as f:
                            angle_data = json.load(f)
                        # ê°ë„ì—ì„œ pose ê³„ì‚° (ì„ì‹œ pose íŒŒì¼ ìƒì„±)
                        angles = angle_data["angles"]
                        coords = angle_data["coords"]
                        # ì—¬ê¸°ì„œ ê°ë„ë¥¼ poseë¡œ ë³€í™˜í•˜ëŠ” ë¡œì§ ì¶”ê°€ í•„ìš”
                        valid_pose_files.append(None)  # ì„ì‹œë¡œ None ì¶”ê°€
            
            print(f"ìœ íš¨í•œ í¬ì¦ˆ: {len(valid_charuco_files)}ê°œ")
            
            if len(valid_charuco_files) < 3:
                print(f"ìœ íš¨í•œ í¬ì¦ˆê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. (í•„ìš”: 3ê°œ ì´ìƒ, í˜„ì¬: {len(valid_charuco_files)}ê°œ)")
                return False
            
            for i, charuco_file in enumerate(valid_charuco_files):
                # ChArUco í¬ì¦ˆ ë¡œë“œ (íƒ€ê²Ÿ -> ì¹´ë©”ë¼) - Eye-to-Hand êµ¬ì¡°
                with open(charuco_file, 'r') as f:
                    charuco_data = json.load(f)
                
                # í’ˆì§ˆ ì ìˆ˜ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©
                quality_score = charuco_data.get("quality_score", 0.5)  # ê¸°ë³¸ê°’ 0.5
                quality_weights.append(quality_score)
                
                # ìƒˆë¡œìš´ í˜•ì‹ì—ì„œëŠ” target2cam ë°ì´í„°ë¥¼ ì‚¬ìš©
                if "rvec_target2cam" in charuco_data:
                    rvec = np.array(charuco_data["rvec_target2cam"])
                    tvec = np.array(charuco_data["tvec_target2cam"])
                else:
                    # ê¸°ì¡´ í˜•ì‹ í˜¸í™˜ì„±ì„ ìœ„í•´ ì—­í–‰ë ¬ ê³„ì‚°
                    rvec_old = np.array(charuco_data["rvec"])
                    tvec_old = np.array(charuco_data["tvec"])
                    R_cam2target, _ = cv2.Rodrigues(rvec_old)
                    R_target2cam = R_cam2target.T
                    t_target2cam = -R_target2cam @ tvec_old
                    rvec = cv2.Rodrigues(R_target2cam)[0]
                    tvec = t_target2cam
                
                # íšŒì „ ë²¡í„°ë¥¼ íšŒì „ í–‰ë ¬ë¡œ ë³€í™˜
                R_cam2target, _ = cv2.Rodrigues(rvec)
                
                # ì¹´ë©”ë¼ -> íƒ€ê²Ÿ (ë¯¸í„° ë‹¨ìœ„) - Eye-to-Hand êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
                T_cam2target = np.eye(4)
                T_cam2target[:3, :3] = R_cam2target.T  # ì—­í–‰ë ¬
                T_cam2target[:3, 3] = -R_cam2target.T @ tvec.flatten()  # ì—­ë³€í™˜
                T_cam2target_list.append(T_cam2target)
                
                # ë¡œë´‡ í¬ì¦ˆ ì²˜ë¦¬ (pose íŒŒì¼ì´ ìˆëŠ” ê²½ìš°)
                if i < len(valid_pose_files) and valid_pose_files[i] is not None:
                    with open(valid_pose_files[i], 'r') as f:
                        pose_data = json.load(f)
                    
                    t_base2ee = np.array(pose_data["t_base2ee"])
                    R_base2ee = np.array(pose_data["R_base2ee"])
                else:
                    # angles íŒŒì¼ì—ì„œ pose ê³„ì‚°
                    angle_files_found = list(self.angles_dir.glob(f"{charuco_file.stem.split('_')[0]}_*_angles.json"))
                    if angle_files_found:
                        with open(angle_files_found[0], 'r') as f:
                            angle_data = json.load(f)
                        
                        coords = angle_data["coords"]
                        # coordsì—ì„œ pose ê³„ì‚° (ê°„ë‹¨í•œ ë³€í™˜)
                        position = np.array(coords[0:3]) / 1000.0  # mmë¥¼ më¡œ ë³€í™˜
                        rx, ry, rz = np.radians(coords[3:6])
                        R_base2ee = self._euler_to_rotation_matrix(rx, ry, rz)
                        t_base2ee = position.reshape(3, 1)
                    else:
                        print(f"í¬ì¦ˆ {i}ì— ëŒ€í•œ ê°ë„ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        continue
                
                # ë¡œë´‡ ë² ì´ìŠ¤ -> End-Effector (ë¯¸í„° ë‹¨ìœ„)
                T_base2ee = np.eye(4)
                T_base2ee[:3, :3] = R_base2ee
                T_base2ee[:3, 3] = t_base2ee.flatten()
                T_base2ee_list.append(T_base2ee)
                
                # ë””ë²„ê·¸ ì •ë³´
                detection_method = charuco_data.get("detection_method", "unknown")
                distance_mm = charuco_data.get("distance_mm", 0)
                print(f"í¬ì¦ˆ {i+1}: {detection_method}, ê±°ë¦¬={distance_mm:.1f}mm, í’ˆì§ˆ={quality_score:.3f}")
            
            # í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì •ê·œí™”
            quality_weights = np.array(quality_weights)
            quality_weights = quality_weights / np.sum(quality_weights)  # í•©ì´ 1ì´ ë˜ë„ë¡ ì •ê·œí™”
            
            print(f"\ní’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜:")
            for i, (weight, score) in enumerate(zip(quality_weights, quality_weights * np.sum(quality_weights))):
                print(f"  í¬ì¦ˆ {i+1}: í’ˆì§ˆ={score:.3f}, ê°€ì¤‘ì¹˜={weight:.3f}")
            
            # Eye-to-Hand calibration ìˆ˜í–‰ (ê°€ì¤‘ì¹˜ í¬í•¨)
            print("\nEye-to-Hand calibration ê³„ì‚° ì¤‘...")
            R_cam2base, t_cam2base = self._solve_eye_to_hand_calibration(
                T_cam2target_list, T_base2ee_list, quality_weights
            )
            
            if R_cam2base is not None:
                # ê²°ê³¼ëŠ” ë¯¸í„° ë‹¨ìœ„ë¡œ ë°˜í™˜ë¨
                t_cam2base_norm = np.linalg.norm(t_cam2base)
                print(f"t_cam2base í¬ê¸° (m): {t_cam2base_norm:.3f}")
                
                # ë¯¸í„° ë‹¨ìœ„ë¡œ ì €ì¥ (í‘œì¤€ ë‹¨ìœ„)
                print(f"ë‹¨ìœ„: ë¯¸í„° (í‘œì¤€ ë‹¨ìœ„)")
                
                # ë³€í™˜ í–‰ë ¬ ìƒì„±
                T_cam2base = np.eye(4)
                T_cam2base[:3, :3] = R_cam2base
                T_cam2base[:3, 3] = t_cam2base.flatten()
                
                # ê²°ê³¼ ì €ì¥ (ë¯¸í„° ë‹¨ìœ„)
                calibration_result = {
                    "R_cam2base": R_cam2base.tolist(),
                    "t_cam2base": t_cam2base.flatten().tolist(),  # ë¯¸í„° ë‹¨ìœ„ë¡œ ì €ì¥
                    "num_poses": len(T_base2ee_list),
                    "timestamp": datetime.now().isoformat(),
                    "description": "ì¹´ë©”ë¼ -> ë¡œë´‡ ë² ì´ìŠ¤ ë³€í™˜ í–‰ë ¬ (Eye-to-Hand calibration, ë¯¸í„° ë‹¨ìœ„, í’ˆì§ˆ ê°€ì¤‘ì¹˜ ì ìš©)",
                    "coordinate_transformation": {
                        "applied": False,
                        "description": "ì¶•ë³´ì • ë¯¸ì ìš© - ì›ë³¸ ì¹´ë©”ë¼ ì¢Œí‘œê³„ ìœ ì§€"
                    },
                    "quality_weighting": {
                        "applied": True,
                        "description": "í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì ìš©",
                        "weights": quality_weights.tolist(),
                        "quality_scores": (quality_weights * np.sum(quality_weights)).tolist()
                    }
                }
                
                result_path = self.save_dir / "cam2base.json"
                with open(result_path, 'w') as f:
                    json.dump(calibration_result, f, indent=2)
                
                # numpy ë°°ì—´ì„ ì•ˆì „í•˜ê²Œ floatë¡œ ë³€í™˜
                t_x = float(t_cam2base.flatten()[0])
                t_y = float(t_cam2base.flatten()[1])
                t_z = float(t_cam2base.flatten()[2])
                
                print(f"âœ… Eye-to-Hand ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì™„ë£Œ!")
                print(f"ê²°ê³¼ ì €ì¥: {result_path}")
                print(f"ì¹´ë©”ë¼ -> ë¡œë´‡ ë² ì´ìŠ¤ ë³€í™˜ í–‰ë ¬:")
                print(f"R:\n{R_cam2base}")
                print(f"t (m): [{t_x:.3f}, {t_y:.3f}, {t_z:.3f}]")
                print(f"í’ˆì§ˆ ê°€ì¤‘ì¹˜ ì ìš©ë¨: {len(quality_weights)}ê°œ í¬ì¦ˆ")
                
                return True
            else:
                print("âŒ ë³€í™˜ í–‰ë ¬ ê³„ì‚° ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            print(f"ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    

    
    def _solve_eye_to_hand_calibration(self, T_cam2target_list, T_base2ee_list, weights=None):
        """
        Eye-to-Hand calibration ë°©ì •ì‹ì„ í•´ê²°í•©ë‹ˆë‹¤.
        
        ë…¼ë¦¬ êµ¬ì¡°:
        T_base2target = T_base2ee (eeì™€ targetì´ ê³ ì • ê´€ê³„)
        T_base2target = T_base2cam * T_cam2target
        T_base2cam = T_base2target * T_cam2target.inverse()
        T_cam2base = T_cam2target * T_base2ee.inverse()
        
        Args:
            T_cam2target_list: ì¹´ë©”ë¼ -> íƒ€ê²Ÿ ë³€í™˜ ë¦¬ìŠ¤íŠ¸ (ë¯¸í„° ë‹¨ìœ„)
            T_base2ee_list: ë² ì´ìŠ¤ -> End-Effector ë³€í™˜ ë¦¬ìŠ¤íŠ¸ (ë¯¸í„° ë‹¨ìœ„)
            weights: í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ê· ë“± ê°€ì¤‘ì¹˜)
            
        Returns:
            Tuple[Optional[np.ndarray], Optional[np.ndarray]]: (R_cam2base, t_cam2base) - ë¯¸í„° ë‹¨ìœ„
        """
        try:
            # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
            if len(T_cam2target_list) < 3:
                print("ì¶©ë¶„í•œ í¬ì¦ˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None, None
            
            print(f"Eye-to-Hand calibration ê³„ì‚° ì¤‘... (ì´ {len(T_cam2target_list)}ê°œ í¬ì¦ˆ)")
            
            # ê° í¬ì¦ˆì—ì„œ T_cam2base ê³„ì‚°
            R_cam2base_list = []
            t_cam2base_list = []
            
            for i, (T_cam2target, T_base2ee) in enumerate(zip(T_cam2target_list, T_base2ee_list)):
                try:
                    # T_cam2base = T_cam2target * T_base2ee.inverse()
                    T_base2ee_inv = np.linalg.inv(T_base2ee)
                    T_cam2base = T_cam2target @ T_base2ee_inv
                    
                    R_cam2base = T_cam2base[:3, :3]
                    t_cam2base = T_cam2base[:3, 3]
                    
                    R_cam2base_list.append(R_cam2base)
                    t_cam2base_list.append(t_cam2base)
                    
                    print(f"í¬ì¦ˆ {i+1}: T_cam2base ê³„ì‚° ì™„ë£Œ")
                    
                except Exception as e:
                    print(f"í¬ì¦ˆ {i+1} ê³„ì‚° ì‹¤íŒ¨: {e}")
                    continue
            
            if len(R_cam2base_list) < 3:
                print("ìœ íš¨í•œ ê³„ì‚° ê²°ê³¼ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
                return None, None
            
            # ê°€ì¤‘ì¹˜ ì²˜ë¦¬
            if weights is None or len(weights) != len(R_cam2base_list):
                # ê· ë“± ê°€ì¤‘ì¹˜
                weights = np.ones(len(R_cam2base_list)) / len(R_cam2base_list)
                print("ê· ë“± ê°€ì¤‘ì¹˜ ì ìš©")
            else:
                # ê°€ì¤‘ì¹˜ ì •ê·œí™” (í•©ì´ 1ì´ ë˜ë„ë¡)
                weights = np.array(weights, dtype=float)
                weights = weights / np.sum(weights)
                print(f"í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì ìš© (ì´ {len(weights)}ê°œ)")
            
            # 1. í‰í–‰ì´ë™ ê°€ì¤‘í‰ê·  (ì§ì ‘ ê³„ì‚°)
            t_cam2base_avg = np.zeros(3)
            for i, (t_cam2base, weight) in enumerate(zip(t_cam2base_list, weights)):
                t_cam2base_avg += weight * t_cam2base
                print(f"  í¬ì¦ˆ {i+1}: t ê°€ì¤‘ì¹˜={weight:.3f}, t={t_cam2base}")
            
            print(f"ê°€ì¤‘í‰ê·  t_cam2base: {t_cam2base_avg}")
            
            # 2. íšŒì „ í–‰ë ¬ ì¿¼í„°ë‹ˆì–¸ ê°€ì¤‘í‰ê· 
            print("ì¿¼í„°ë‹ˆì–¸ ê°€ì¤‘í‰ê· ìœ¼ë¡œ íšŒì „ í–‰ë ¬ ê³„ì‚° ì¤‘...")
            
            # ê° íšŒì „ í–‰ë ¬ì„ ì¿¼í„°ë‹ˆì–¸ìœ¼ë¡œ ë³€í™˜
            quaternions = []
            for i, R in enumerate(R_cam2base_list):
                try:
                    # íšŒì „ í–‰ë ¬ì„ ì¿¼í„°ë‹ˆì–¸ìœ¼ë¡œ ë³€í™˜
                    q = self._rotation_matrix_to_quaternion(R)
                    quaternions.append(q)
                    print(f"  í¬ì¦ˆ {i+1}: ì¿¼í„°ë‹ˆì–¸ ë³€í™˜ ì™„ë£Œ")
                except Exception as e:
                    print(f"  í¬ì¦ˆ {i+1}: ì¿¼í„°ë‹ˆì–¸ ë³€í™˜ ì‹¤íŒ¨ - {e}")
                    # ì‹¤íŒ¨í•œ ê²½ìš° ë‹¨ìœ„ ì¿¼í„°ë‹ˆì–¸ ì‚¬ìš©
                    quaternions.append(np.array([1.0, 0.0, 0.0, 0.0]))
            
            # ì¿¼í„°ë‹ˆì–¸ ê°€ì¤‘í‰ê·  ê³„ì‚°
            if len(quaternions) > 0:
                R_cam2base_final = self._weighted_average_quaternions(quaternions, weights)
                print("ì¿¼í„°ë‹ˆì–¸ ê°€ì¤‘í‰ê·  ì™„ë£Œ")
            else:
                print("ì¿¼í„°ë‹ˆì–¸ ë³€í™˜ ì‹¤íŒ¨ë¡œ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©")
                # ê¸°ì¡´ ë°©ì‹ (ìš”ì†Œë³„ ê°€ì¤‘í‰ê·  í›„ SVD ì •ê·œí™”)
                R_cam2base_avg = np.zeros((3, 3))
                for i, (R, weight) in enumerate(zip(R_cam2base_list, weights)):
                    R_cam2base_avg += weight * R
                
                # íšŒì „ í–‰ë ¬ì„ ì§êµ í–‰ë ¬ë¡œ ì •ê·œí™”
                U, _, Vt = np.linalg.svd(R_cam2base_avg)
                R_cam2base_final = U @ Vt
                print("ê¸°ì¡´ ë°©ì‹ (ìš”ì†Œë³„ ê°€ì¤‘í‰ê·  + SVD) ì‚¬ìš©")
            
            print(f"ìµœì¢… R_cam2base:\n{R_cam2base_final}")
            print(f"ìµœì¢… t_cam2base: {t_cam2base_avg}")
            
            return R_cam2base_final, t_cam2base_avg
            
        except Exception as e:
            print(f"Eye-to-Hand calibration ê³„ì‚° ì˜¤ë¥˜: {e}")
            return None, None
    
    def _rotation_matrix_to_quaternion(self, R: np.ndarray) -> np.ndarray:
        """
        íšŒì „ í–‰ë ¬ì„ ì¿¼í„°ë‹ˆì–¸ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        Args:
            R: 3x3 íšŒì „ í–‰ë ¬
            
        Returns:
            np.ndarray: [w, x, y, z] í˜•íƒœì˜ ì¿¼í„°ë‹ˆì–¸
        """
        # íšŒì „ í–‰ë ¬ì˜ ëŒ€ê°ì„  ìš”ì†Œë“¤ì˜ í•©
        trace = np.trace(R)
        
        if trace > 0:
            S = np.sqrt(trace + 1.0) * 2
            w = 0.25 * S
            x = (R[2, 1] - R[1, 2]) / S
            y = (R[0, 2] - R[2, 0]) / S
            z = (R[1, 0] - R[0, 1]) / S
        elif R[0, 0] > R[1, 1] and R[0, 0] > R[2, 2]:
            S = np.sqrt(1.0 + R[0, 0] - R[1, 1] - R[2, 2]) * 2
            w = (R[2, 1] - R[1, 2]) / S
            x = 0.25 * S
            y = (R[0, 1] + R[1, 0]) / S
            z = (R[0, 2] + R[2, 0]) / S
        elif R[1, 1] > R[2, 2]:
            S = np.sqrt(1.0 + R[1, 1] - R[0, 0] - R[2, 2]) * 2
            w = (R[0, 2] - R[2, 0]) / S
            x = (R[0, 1] + R[1, 0]) / S
            y = 0.25 * S
            z = (R[1, 2] + R[2, 1]) / S
        else:
            S = np.sqrt(1.0 + R[2, 2] - R[0, 0] - R[1, 1]) * 2
            w = (R[1, 0] - R[0, 1]) / S
            x = (R[0, 2] + R[2, 0]) / S
            y = (R[1, 2] + R[2, 1]) / S
            z = 0.25 * S
        
        return np.array([w, x, y, z])
    
    def _quaternion_to_rotation_matrix(self, q: np.ndarray) -> np.ndarray:
        """
        ì¿¼í„°ë‹ˆì–¸ì„ íšŒì „ í–‰ë ¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        Args:
            q: [w, x, y, z] í˜•íƒœì˜ ì¿¼í„°ë‹ˆì–¸
            
        Returns:
            np.ndarray: 3x3 íšŒì „ í–‰ë ¬
        """
        w, x, y, z = q
        
        # ì¿¼í„°ë‹ˆì–¸ ì •ê·œí™”
        norm = np.sqrt(w*w + x*x + y*y + z*z)
        if norm > 0:
            w, x, y, z = w/norm, x/norm, y/norm, z/norm
        
        # íšŒì „ í–‰ë ¬ ê³„ì‚°
        R = np.array([
            [1-2*y*y-2*z*z,     2*x*y-2*w*z,     2*x*z+2*w*y],
            [    2*x*y+2*w*z, 1-2*x*x-2*z*z,     2*y*z-2*w*x],
            [    2*x*z-2*w*y,     2*y*z+2*w*x, 1-2*x*x-2*y*y]
        ])
        
        return R
    
    def _weighted_average_quaternions(self, quaternions: List[np.ndarray], weights: np.ndarray) -> np.ndarray:
        """
        ì¿¼í„°ë‹ˆì–¸ë“¤ì˜ ê°€ì¤‘í‰ê· ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        Args:
            quaternions: ì¿¼í„°ë‹ˆì–¸ ë¦¬ìŠ¤íŠ¸
            weights: ê°€ì¤‘ì¹˜ ë°°ì—´ (í•©ì´ 1)
            
        Returns:
            np.ndarray: ê°€ì¤‘í‰ê·  ì¿¼í„°ë‹ˆì–¸
        """
        if len(quaternions) == 0:
            return np.array([1.0, 0.0, 0.0, 0.0])
        
        if len(quaternions) == 1:
            return quaternions[0]
        
        # ê°€ì¤‘í•© ê³„ì‚° (ì¿¼í„°ë‹ˆì–¸ì˜ ë¶€í˜¸ë¥¼ ë§ì¶¤)
        weighted_sum = np.zeros(4)
        
        for i, (q, weight) in enumerate(zip(quaternions, weights)):
            # ì²« ë²ˆì§¸ ì¿¼í„°ë‹ˆì–¸ê³¼ì˜ ë‚´ì ì„ í™•ì¸í•˜ì—¬ ë¶€í˜¸ ê²°ì •
            if i == 0:
                weighted_sum += weight * q
            else:
                # ì²« ë²ˆì§¸ ì¿¼í„°ë‹ˆì–¸ê³¼ì˜ ë‚´ì ì´ ìŒìˆ˜ë©´ ë¶€í˜¸ ë°˜ì „
                dot_product = np.dot(quaternions[0], q)
                if dot_product < 0:
                    weighted_sum += weight * (-q)
                else:
                    weighted_sum += weight * q
        
        # ì •ê·œí™”
        norm = np.linalg.norm(weighted_sum)
        if norm > 0:
            weighted_sum = weighted_sum / norm
        
        # ì¿¼í„°ë‹ˆì–¸ì„ íšŒì „ í–‰ë ¬ë¡œ ë³€í™˜
        R_final = self._quaternion_to_rotation_matrix(weighted_sum)
        
        return R_final
    
    def test_calibration(self) -> bool:
        """
        ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê²°ê³¼ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
        """
        print("\n=== Eye-to-Hand ìº˜ë¦¬ë¸Œë ˆì´ì…˜ í…ŒìŠ¤íŠ¸ ===")
        
        try:
            # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê²°ê³¼ ë¡œë“œ
            result_path = self.save_dir / "cam2base.json"
            if not result_path.exists():
                print("ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            with open(result_path, 'r') as f:
                calibration_data = json.load(f)
            
            R_cam2base = np.array(calibration_data["R_cam2base"])
            t_cam2base = np.array(calibration_data["t_cam2base"])  # ë¯¸í„° ë‹¨ìœ„
            
            print("ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê²°ê³¼:")
            print(f"R_cam2base:\n{R_cam2base}")
            
            # ì¢Œí‘œì¶• ë³€í™˜ ì •ë³´ í™•ì¸
            if "coordinate_transformation" in calibration_data:
                coord_info = calibration_data["coordinate_transformation"]
                if coord_info.get("applied", False):
                    print(f"\nì¢Œí‘œì¶• ë³€í™˜ ì ìš©ë¨:")
                    print(f"ì„¤ëª…: {coord_info['description']}")
                    if "transformation_matrix" in coord_info:
                        print(f"ë³€í™˜ í–‰ë ¬:\n{np.array(coord_info['transformation_matrix'])}")
                else:
                    print(f"\nì¢Œí‘œì¶• ë³€í™˜ ë¯¸ì ìš©:")
                    print(f"ì„¤ëª…: {coord_info['description']}")
            
            # í’ˆì§ˆ ê°€ì¤‘ì¹˜ ì •ë³´ í™•ì¸
            if "quality_weighting" in calibration_data:
                quality_info = calibration_data["quality_weighting"]
                if quality_info.get("applied", False):
                    print(f"\ní’ˆì§ˆ ê°€ì¤‘ì¹˜ ì ìš©ë¨:")
                    print(f"ì„¤ëª…: {quality_info['description']}")
                    
                    # ê°€ì¤‘ì¹˜ ì •ë³´ í‘œì‹œ
                    weights = quality_info.get("weights", [])
                    quality_scores = quality_info.get("quality_scores", [])
                    
                    if weights and quality_scores:
                        print(f"í’ˆì§ˆ ì ìˆ˜ ë° ê°€ì¤‘ì¹˜:")
                        for i, (score, weight) in enumerate(zip(quality_scores, weights)):
                            print(f"  í¬ì¦ˆ {i+1}: í’ˆì§ˆ={score:.3f}, ê°€ì¤‘ì¹˜={weight:.3f}")
                else:
                    print(f"\ní’ˆì§ˆ ê°€ì¤‘ì¹˜ ë¯¸ì ìš©")
            
            # t_cam2base ë‹¨ìœ„ í™•ì¸
            t_cam2base_norm = np.linalg.norm(t_cam2base)
            print(f"\nt_cam2base í¬ê¸° (m): {t_cam2base_norm:.3f}")
            
            # numpy ë°°ì—´ì„ ì•ˆì „í•˜ê²Œ floatë¡œ ë³€í™˜
            t_x = float(t_cam2base.flatten()[0])
            t_y = float(t_cam2base.flatten()[1])
            t_z = float(t_cam2base.flatten()[2])
            print(f"t_cam2base (m): [{t_x:.3f}, {t_y:.3f}, {t_z:.3f}]")
            
            # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìº¡ì²˜
            print("\ní…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìº¡ì²˜ ì¤‘...")
            color_path, depth_path, intrinsics_path = capture_d455_images(
                save_dir=str(self.save_dir),
                rgb_size=(848, 480),
                depth_size=(848, 480)
            )
            
            # ì¹´ë©”ë¼ ë‚´ë¶€ íŒŒë¼ë¯¸í„° ë¡œë“œ
            camera_matrix, dist_coeffs = self._load_camera_intrinsics(intrinsics_path)
            
            # ChArUco ê²€ì¶œ
            image = cv2.imread(color_path)
            charuco_pose = self.detect_charuco_pose(image, camera_matrix, dist_coeffs)
            
            if charuco_pose is None:
                print("í…ŒìŠ¤íŠ¸ì—ì„œ ChArUco ê²€ì¶œ ì‹¤íŒ¨")
                return False
            
            rvec, tvec, _, _ = charuco_pose
            
            # ì¹´ë©”ë¼ -> ChArUco ë³€í™˜ (ë¯¸í„° ë‹¨ìœ„)
            R_cam2charuco, _ = cv2.Rodrigues(rvec)
            t_cam2charuco = tvec  # ë¯¸í„° ë‹¨ìœ„
            
            # í˜„ì¬ ë¡œë´‡ í¬ì¦ˆ (ë² ì´ìŠ¤ -> Gripper)
            robot_coords = self.robot.get_coords()
            robot_position_mm = np.array(robot_coords[0:3])
            robot_rotation_deg = np.array(robot_coords[3:6])
            
            # numpy ë°°ì—´ì„ ì•ˆì „í•˜ê²Œ floatë¡œ ë³€í™˜
            robot_x = float(robot_position_mm.flatten()[0])
            robot_y = float(robot_position_mm.flatten()[1])
            robot_z = float(robot_position_mm.flatten()[2])
            robot_rx = float(robot_rotation_deg.flatten()[0])
            robot_ry = float(robot_rotation_deg.flatten()[1])
            robot_rz = float(robot_rotation_deg.flatten()[2])
            
            print(f"\nì‹¤ì œ ë¡œë´‡ ìœ„ì¹˜ (ë² ì´ìŠ¤ ê¸°ì¤€):")
            print(f"End-Effector ìœ„ì¹˜: {robot_x:.1f}, {robot_y:.1f}, {robot_z:.1f}mm")
            print(f"End-Effector íšŒì „: {robot_rx:.1f}, {robot_ry:.1f}, {robot_rz:.1f}ë„")
            
            # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì„ í†µí•œ ê³„ì‚° (Eye-to-Hand êµ¬ì¡°)
            # ì¹´ë©”ë¼ -> ë¡œë´‡ ë² ì´ìŠ¤ ë³€í™˜ì„ ì‚¬ìš©í•˜ì—¬ ChArUco ìœ„ì¹˜ ê³„ì‚°
            # ì¹´ë©”ë¼ì—ì„œ ChArUcoê¹Œì§€ì˜ ë³€í™˜ì„ ë¡œë´‡ ë² ì´ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜
            charuco_in_base = R_cam2base @ t_cam2charuco + t_cam2base.reshape(3, 1)
            charuco_in_base_mm = charuco_in_base.flatten() * 1000  # m -> mm (í‘œì‹œìš©)
            
            # numpy ë°°ì—´ì„ ì•ˆì „í•˜ê²Œ floatë¡œ ë³€í™˜
            charuco_x = float(charuco_in_base_mm.flatten()[0])
            charuco_y = float(charuco_in_base_mm.flatten()[1])
            charuco_z = float(charuco_in_base_mm.flatten()[2])
            
            print(f"\nìº˜ë¦¬ë¸Œë ˆì´ì…˜ì„ í†µí•œ ChArUco ìœ„ì¹˜ ê³„ì‚°:")
            print(f"ì¹´ë©”ë¼ì—ì„œ ê³„ì‚°ëœ ChArUco ìœ„ì¹˜ (ë¡œë´‡ ë² ì´ìŠ¤ ê¸°ì¤€): {charuco_x:.1f}, {charuco_y:.1f}, {charuco_z:.1f}mm")
            
            # ê±°ë¦¬ ê³„ì‚°
            distance = np.linalg.norm(charuco_in_base_mm)
            distance_float = float(distance)
            print(f"ì¹´ë©”ë¼ì—ì„œ ê³„ì‚°ëœ ê±°ë¦¬: {distance_float:.1f}mm")
            
            # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¶œë ¥
            print(f"\nìº˜ë¦¬ë¸Œë ˆì´ì…˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
            print(f"ì¹´ë©”ë¼ -> ë¡œë´‡ ë² ì´ìŠ¤ ë³€í™˜ í–‰ë ¬ì´ ì„±ê³µì ìœ¼ë¡œ ê³„ì‚°ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print(f"ì´ì œ ì¹´ë©”ë¼ì—ì„œ ê°ì§€ëœ ê°ì²´ì˜ í”½ì…€ ì¢Œí‘œë¥¼ ë¡œë´‡ ë² ì´ìŠ¤ ê¸°ì¤€ 3D ì¢Œí‘œë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            return True
            
        except Exception as e:
            print(f"ìº˜ë¦¬ë¸Œë ˆì´ì…˜ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
            return False
    


def main():
    """
    ë©”ì¸ í•¨ìˆ˜
    """
    print("Eye-to-Hand Calibration for MyCobot280")
    print("=" * 50)
    print("âœ¨ ìƒˆë¡œìš´ ê¸°ëŠ¥: í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì ìš©")
    print("   - ê° í¬ì¦ˆì˜ quality_scoreë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©")
    print("   - íšŒì „: ì¿¼í„°ë‹ˆì–¸ ê°€ì¤‘í‰ê· ìœ¼ë¡œ ë” ì •í™•í•œ ê²°ê³¼")
    print("   - í‰í–‰ì´ë™: ê°€ì¤‘í‰ê· ìœ¼ë¡œ ì¡ìŒì— ê°•ê±´í•œ ê²°ê³¼")
    print("ğŸ”’ ì•ˆì „ ê¸°ëŠ¥: ìµœì†Œ 6ê°œ ì½”ë„ˆ ê²€ì¶œ ì‹œì—ë§Œ í¬ì¦ˆ ì¶”ì •")
    print("   - OpenCV ì˜¤ë¥˜ ë°©ì§€ ë° ì•ˆì •ì ì¸ ìº˜ë¦¬ë¸Œë ˆì´ì…˜")
    print("=" * 50)
    
    # ìº˜ë¦¬ë¸Œë ˆì´í„° ì´ˆê¸°í™” (ì‹¤ì œ ë³´ë“œ í¬ê¸°ì— ë§ì¶¤)
    calibrator = EyeToHandCalibrator(
        robot_port="/dev/ttyACM1",
        robot_baud=115200,
        save_dir="/home/ros/llm_robot/data/Calibration/Eye-to-Hand12",
        charuco_squares_x=7, 
        charuco_squares_y=11,   
        charuco_square_length=27.0, # mm (ì‹¤ì œ ì¸¡ì •ê°’)
        charuco_marker_length=16.0, # mm (ì‹¤ì œ ì¸¡ì •ê°’)
        min_charuco_corners=20,  # ìµœì†Œ ChArUco ì½”ë„ˆ ê°œìˆ˜ (ë” ì™„í™”)
        min_detection_confidence=0.2,  # ìµœì†Œ ê²€ì¶œ ì‹ ë¢°ë„ (ë”ìš± ì™„í™”)
        max_distance_threshold=0.7)  # ìµœëŒ€ ê±°ë¦¬ ì„ê³„ê°’ (ë¯¸í„°)
    
    
    try:
        print("\n=== ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì˜µì…˜ ===")
        print("1. ì‹¤ì‹œê°„ ê°ë„ ìˆ˜ì§‘ ëª¨ë“œ (ì¹´ë©”ë¼ í™”ë©´ í¬í•¨)")
        print("2. ìë™ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ëª¨ë“œ (ì €ì¥ëœ ê°ë„ ì‚¬ìš©)")
        print("3. ìˆ˜ë™ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ëª¨ë“œ (ê¸°ì¡´ ë°©ì‹)")
        print("4. í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰")
        print("5. ì¢…ë£Œ")
        
        choice = input("ì„ íƒ (1/2/3/4): ").strip()
        
        if choice == "1":
            # ì‹¤ì‹œê°„ ê°ë„ ìˆ˜ì§‘ ëª¨ë“œ (ì¹´ë©”ë¼ í™”ë©´ í¬í•¨)
            num_poses = int(input("\nìˆ˜ì§‘í•  í¬ì¦ˆ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¶Œì¥: 25-30): ") or "25")
            print(f"\n{num_poses}ê°œì˜ í¬ì¦ˆë¡œ ì‹¤ì‹œê°„ ê°ë„ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            print("ì¹´ë©”ë¼ í™”ë©´ì´ í‘œì‹œë˜ë©°, ChArUco ê²€ì¶œ ìƒíƒœë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            print("ê° í¬ì¦ˆì—ì„œ 's'ë¥¼ ëˆŒëŸ¬ ì €ì¥í•˜ê±°ë‚˜ 'q'ë¥¼ ëˆŒëŸ¬ ì¢…ë£Œí•˜ì„¸ìš”.")
            
            collection_success = calibrator.collect_angles_with_camera_feed(num_poses)
            
            if collection_success:
                print("\nğŸ‰ ì‹¤ì‹œê°„ ê°ë„ ìˆ˜ì§‘ ì™„ë£Œ!")
                print("\nğŸ“Š ìˆ˜ì§‘ëœ ë°ì´í„°ë¡œ cam2ee.jsonì„ ìë™ ê³„ì‚°í•©ë‹ˆë‹¤...")
                calibration_success = calibrator.calculate_transformation_matrix()
                if calibration_success:
                    print("\nğŸ‰ Eye-to-Hand ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì™„ë£Œ!")
                    test_choice = input("\nìº˜ë¦¬ë¸Œë ˆì´ì…˜ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower().strip()
                    if test_choice == 'y':
                        calibrator.test_calibration()
                else:
                    print("\nâŒ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì‹¤íŒ¨")
            else:
                print("\nâŒ ê°ë„ ìˆ˜ì§‘ ì‹¤íŒ¨")
                
        elif choice == "2":
            # ìë™ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ëª¨ë“œ
            print("\nì €ì¥ëœ ê°ë„ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ìë™ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            calibration_success = calibrator.perform_automatic_calibration()
            
            if calibration_success:
                print("\nï¿½ï¿½ Cam-to-Gripper ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì™„ë£Œ!")
                test_choice = input("\nìº˜ë¦¬ë¸Œë ˆì´ì…˜ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower().strip()
                if test_choice == 'y':
                    calibrator.test_calibration()
            else:
                print("\nâŒ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì‹¤íŒ¨")
                
        elif choice == "3":
            # ìˆ˜ë™ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ëª¨ë“œ (ê¸°ì¡´ ë°©ì‹)
            num_poses = int(input("\nìº¡ì²˜í•  í¬ì¦ˆ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¶Œì¥: 25-30): ") or "25")
            print(f"\n{num_poses}ê°œì˜ í¬ì¦ˆë¡œ ìˆ˜ë™ Eye-to-Hand ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            calibration_success = calibrator.perform_manual_calibration(num_poses)
            
            if calibration_success:
                print("\nğŸ‰ Eye-to-Hand ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì™„ë£Œ!")
                test_choice = input("\nìº˜ë¦¬ë¸Œë ˆì´ì…˜ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower().strip()
                if test_choice == 'y':
                    calibrator.test_calibration()
            else:
                print("\nâŒ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì‹¤íŒ¨")
                
        elif choice == "4":
            # í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰
            calibrator.test_calibration()
        else:
            print("\nì¢…ë£Œí•©ë‹ˆë‹¤.")
            
    except KeyboardInterrupt:
        print("\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()